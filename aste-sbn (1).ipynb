{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7813584,"sourceType":"datasetVersion","datasetId":4576911},{"sourceId":7814010,"sourceType":"datasetVersion","datasetId":4577262},{"sourceId":7814021,"sourceType":"datasetVersion","datasetId":4577271},{"sourceId":7823324,"sourceType":"datasetVersion","datasetId":4583983},{"sourceId":7947056,"sourceType":"datasetVersion","datasetId":4673036},{"sourceId":7949020,"sourceType":"datasetVersion","datasetId":4674493},{"sourceId":7991857,"sourceType":"datasetVersion","datasetId":4704942}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install allennlp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch\n!pip install numpy\n!pip install thop\n!pip install transformers\n!pip install tqdm\n!pip install natasha","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile Attention.py\n\nimport torch\nimport math\nimport torch.nn as nn\nimport numpy as np\nfrom transformers.activations import ACT2FN\n\nclass Dim_Four_Attention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.output = SelfOutput(config)\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 1, 3, 2, 4)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        ):\n        mixed_query_layer = self.query(hidden_states)\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention:\n            if encoder_attention_mask is not None:\n                attention_mask = encoder_attention_mask\n            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n        else:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 1, 3, 2, 4).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        self_outputs = (context_layer,)\n        attention_output = self.output(self_outputs[0], hidden_states)\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n        return outputs\n\n\nclass Attention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.self = SelfAttention(config)\n        self.output = SelfOutput(config)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n    ):\n        self_outputs = self.self(\n            hidden_states,\n            attention_mask,\n            encoder_hidden_states,\n            encoder_attention_mask)\n        attention_output = self.output(self_outputs[0], hidden_states)\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n        return outputs\n\nclass SelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n    ):\n        mixed_query_layer = self.query(hidden_states)\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention:\n            if encoder_attention_mask is not None:\n                attention_mask = encoder_attention_mask\n            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n        else:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        outputs = (context_layer,)\n        return outputs\n\nclass SelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\nclass Output(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\nclass Intermediate(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\ndef masked_softmax(tensor, mask_1, mask_2):\n    \"\"\"\n    Apply a masked softmax on the last dimension of a tensor.\n    The input tensor and mask should be of size (batch, *, sequence_length).\n\n    Args:\n        tensor: The tensor on which the softmax function must be applied along\n            the last dimension.\n        mask: A mask of the same size as the tensor with 0s in the positions of\n            the values that must be masked and 1s everywhere else.\n\n    Returns:\n        A tensor of the same size as the inputs containing the result of the\n        softmax.\n    \"\"\"\n    tensor_shape = tensor.size()\n    reshaped_tensor = tensor.view(-1, tensor_shape[-1])\n\n    # Reshape the mask so it matches the size of the input tensor.\n    while mask_2.dim() < tensor.dim():\n        mask_2 = mask_2.unsqueeze(1)\n    mask_2 = mask_2.expand_as(tensor).contiguous().float()\n    reshaped_mask = mask_2.view(-1, mask_2.size()[-1])\n\n    result = nn.functional.softmax(reshaped_tensor * reshaped_mask, dim=-1)\n    result = result * reshaped_mask\n    # 1e-13 is added to avoid divisions by zero.\n    result = result / (result.sum(dim=-1, keepdim=True) + 1e-13)\n    result = result.view(*tensor_shape)\n\n    while mask_1.dim() < result.dim():\n        mask_1 = mask_1.unsqueeze(2)\n    mask_1 = mask_1.expand_as(result).contiguous().float()\n\n    result_2 = nn.functional.softmax(result * mask_1, dim=-2)\n    result_2 = result_2 * mask_1\n    result_2 = result_2 / (result_2.sum(dim=-2, keepdim=True) + 1e-13)\n    return result_2\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T09:24:45.167536Z","iopub.execute_input":"2024-04-30T09:24:45.167859Z","iopub.status.idle":"2024-04-30T09:24:45.181617Z","shell.execute_reply.started":"2024-04-30T09:24:45.167818Z","shell.execute_reply":"2024-04-30T09:24:45.180708Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Writing Attention.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile data_BIO_loader.py\n\nimport torch\nimport numpy as np\nimport random\nimport json\nfrom transformers import BertTokenizer\nimport tqdm\nfrom tqdm import tqdm\nfrom gcn import make_adj_matrix\nimport spacy\nimport string\n\nvalidity2id = {'none': 0, 'positive': 1, 'negative': 1, 'neutral': 1, 'start': 1}\nsentiment2id = {'none': 0, 'positive': 1, 'negative': 2, 'neutral': 3, 'start': 4}\n\n\n# nlp = spacy.load(\"ru_core_news_md\")\n\n\n\ndef get_spans(tags):\n    '''for BIO tag'''\n    tags = tags.strip().split()\n    length = len(tags)\n    spans = []\n    start = -1\n    for i in range(length):\n        if tags[i].endswith('B'):\n            if start != -1:\n                spans.append([start, i - 1])\n            start = i\n        elif tags[i].endswith('O'):\n            if start != -1:\n                spans.append([start, i - 1])\n                start = -1\n    if start != -1:\n        spans.append([start, length - 1])\n    return spans\n\n\ndef get_subject_labels(tags):\n    '''for BIO tag'''\n\n    label = {}\n    subject_span = get_spans(tags)[0]\n    tags = tags.strip().split()\n    sentence = []\n    for tag in tags:\n        sentence.append(tag.strip().split('\\\\')[0])\n    word = ' '.join(sentence[subject_span[0]:subject_span[1] + 1])\n    label[word] = subject_span\n    return label\n\n\ndef get_object_labels(tags):\n    '''for BIO tag'''\n    label = {}\n    object_spans = get_spans(tags)\n    tags = tags.strip().split()\n    sentence = []\n    for tag in tags:\n        sentence.append(tag.strip().split('\\\\')[0])\n    for object_span in object_spans:\n        word = ' '.join(sentence[object_span[0]:object_span[1] + 1])\n        label[word] = object_span\n    return label\n\n\nclass InputExample(object):\n    def __init__(self, id, text_a, aspect_num, triple_num, all_label=None, text_b=None):\n        \"\"\"Build a InputExample\"\"\"\n        self.id = id\n        self.text_a = text_a\n        self.text_b = text_b\n        self.all_label = all_label\n        self.aspect_num = aspect_num\n        self.triple_num = triple_num\n\n\nclass Instance(object):\n    def __init__(self, sentence_pack, args):\n        triple_dict = {}\n        id = sentence_pack['id']\n        aspect_num = 0\n        for triple in sentence_pack['triples']:\n            aspect = triple['target_tags']\n            opinion = triple['opinion_tags']\n            sentiment = triple['sentiment']\n            subject_label = get_subject_labels(aspect)\n            object_label = get_object_labels(opinion)\n            objects = list(object_label.keys())\n            subject = list(subject_label.keys())[0]\n            aspect_num += len(subject_label)\n            for i, object in enumerate(objects):\n                # 由于数据集的每个triples中aspect只有一个，而opinion可能有多个  需要分开构建\n                word = str(subject) + '|' + str(object)\n                if word not in triple_dict:\n                    triple_dict[word] = []\n                triple_dict[word] = (subject_label[subject], object_label[object], sentiment)\n        examples = InputExample(id=id, text_a=sentence_pack['sentence'], text_b=None, all_label=triple_dict,\n                                aspect_num=aspect_num, triple_num=len(triple_dict))\n        self.examples = examples\n        self.triple_num = len(triple_dict)\n        self.aspect_num = aspect_num\n\n\ndef load_data_instances(sentence_packs, args):\n    instances = list()\n    triples_num = 0\n    aspects_num = 0\n    for i, sentence_pack in enumerate(sentence_packs):\n        instance = Instance(sentence_pack, args)\n        instances.append(instance.examples)\n        triples_num += instance.triple_num\n        aspects_num += instance.aspect_num\n    return instances\n\n\ndef convert_examples_to_features(args, train_instances, max_span_length=8):\n\n    features = []\n    num_aspect = 0\n    num_triple = 0\n    num_opinion = 0\n    differ_opinion_senitment_num = 0\n    differ_aspect_sentiment_num = 0\n    for ex_index, example in enumerate(train_instances):\n        sample = {'id': example.id}\n        sample['tokens'] = example.text_a.split(' ')\n        sample['text_length'] = len(sample['tokens'])\n        sample['triples'] = example.all_label\n        sample['sentence'] = example.text_a\n        \n        aspect = {}\n        opinion = {}\n\n        opinion_reverse = {}\n        aspect_reverse  = {}\n\n        differ_opinion_sentiment = False\n        differ_aspect_sentiment = False\n\n        for triple_name in sample['triples']:\n            aspect_span, opinion_span, sentiment = tuple(sample['triples'][triple_name][0]), tuple(\n                sample['triples'][triple_name][1]), sample['triples'][triple_name][2]\n            num_triple += 1\n            if aspect_span not in aspect:\n                aspect[aspect_span] = sentiment\n                opinion[aspect_span] = [(opinion_span, sentiment)]\n            else:\n                if aspect[aspect_span] != sentiment:\n                    differ_aspect_sentiment = True\n                else:\n                    opinion[aspect_span].append((opinion_span, sentiment))\n\n            if opinion_span not in opinion_reverse:\n                opinion_reverse[opinion_span] = sentiment\n                aspect_reverse[opinion_span] = [(aspect_span, sentiment)]\n            else:\n                '''同一aspect的不同的opinion拥有相同极性，但是'''\n                if opinion_reverse[opinion_span] != sentiment:\n                    differ_opinion_sentiment = True\n                else:\n                    aspect_reverse[opinion_span].append((aspect_span, sentiment))\n        \n        if differ_opinion_sentiment:\n            differ_opinion_senitment_num += 1\n            print(ex_index, 'Single opinion word multi-polarity')\n            continue\n\n        if differ_aspect_sentiment:\n            differ_aspect_sentiment_num += 1\n            print(ex_index, 'Single aspect word multi-polarity')\n            continue\n\n        num_aspect += len(aspect)\n        num_opinion += len(opinion)\n\n        # if len(aspect) != example.aspect_num:\n        #     print('有不同三元组使用重复了aspect:', example.id)\n\n        spans = []\n        span_tokens = []\n\n        spans_aspect_label = []\n        spans_aspect2opinion_label =[]\n        spans_opinion_label = []\n\n        reverse_opinion_label = []\n        reverse_opinion2aspect_label = []\n        reverse_aspect_label = []\n        \n        punk = string.punctuation\n\n        if args.order_input:\n            for i in range(max_span_length):\n                if sample['text_length'] < i:\n                    continue\n                for j in range(sample['text_length'] - i):\n                    if sample['tokens'][j] in punk:\n                        continue\n                    spans.append((j, i + j, i + 1))\n                    span_token = ' '.join(sample['tokens'][j:i + j + 1])\n                    span_tokens.append(span_token)\n                    if (j, i + j) not in aspect:\n                        spans_aspect_label.append(0)\n                    else:\n                        # spans_aspect_label.append(sentiment2id[aspect[(j, i + j)]])\n                        spans_aspect_label.append(validity2id[aspect[(j, i + j)]])\n                    if (j, i + j) not in opinion_reverse:\n                        reverse_opinion_label.append(0)\n                    else:\n                        # reverse_opinion_label.append(sentiment2id[opinion_reverse[(j, i + j)]])\n                        reverse_opinion_label.append(validity2id[opinion_reverse[(j, i + j)]])\n\n        else:\n            for i in range(sample['text_length']):\n                for j in range(i, min(sample['text_length'], i + max_span_length)):\n                    spans.append((i, j, j - i + 1))\n                    span_token = ' '.join(sample['tokens'][i:j + 1])\n                    span_tokens.append(span_token)\n                    if (i, j) not in aspect:\n                        spans_aspect_label.append(0)\n                    else:\n                        spans_aspect_label.append(validity2id[aspect[(i, j)]])\n                    if (i, j) not in opinion_reverse:\n                        reverse_opinion_label.append(0)\n                    else:\n                        reverse_opinion_label.append(validity2id[opinion_reverse[(i, j)]])\n\n\n        assert len(span_tokens) == len(spans)\n        for key_aspect in opinion:\n            opinion_list = []\n            sentiment_opinion = []\n            spans_aspect2opinion_label.append(key_aspect)\n            for opinion_span_2_aspect in opinion[key_aspect]:\n                opinion_list.append(opinion_span_2_aspect[0])\n                sentiment_opinion.append(opinion_span_2_aspect[1])\n            assert len(set(sentiment_opinion)) == 1\n            opinion_label2triple = []\n            for i in spans:\n                if (i[0], i[1]) not in opinion_list:\n                    opinion_label2triple.append(0)\n                else:\n                    opinion_label2triple.append(sentiment2id[sentiment_opinion[0]])\n            spans_opinion_label.append(opinion_label2triple)\n\n        for opinion_key in aspect_reverse:\n            aspect_list = []\n            sentiment_aspect = []\n            reverse_opinion2aspect_label.append(opinion_key)\n            for aspect_span_2_opinion in aspect_reverse[opinion_key]:\n                aspect_list.append(aspect_span_2_opinion[0])\n                sentiment_aspect.append(aspect_span_2_opinion[1])\n            assert len(set(sentiment_aspect)) == 1\n            aspect_label2triple = []\n            for i in spans:\n                if (i[0], i[1]) not in aspect_list:\n                    aspect_label2triple.append(0)\n                else:\n                    aspect_label2triple.append(sentiment2id[sentiment_aspect[0]])\n            reverse_aspect_label.append(aspect_label2triple)\n\n        sample['aspect_num'] = len(spans_opinion_label)\n        sample['spans_aspect2opinion_label'] = spans_aspect2opinion_label\n        sample['reverse_opinion_num'] = len(reverse_aspect_label)\n        sample['reverse_opinion2aspect_label'] = reverse_opinion2aspect_label\n\n        if args.random_shuffle != 0:\n            np.random.seed(args.random_shuffle)\n            shuffle_ix = np.random.permutation(np.arange(len(spans)))\n            spans_np = np.array(spans)[shuffle_ix]\n            span_tokens_np = np.array(span_tokens)[shuffle_ix]\n            '''双向同顺序打乱'''\n            spans_aspect_label_np = np.array(spans_aspect_label)[shuffle_ix]\n            reverse_opinion_label_np = np.array(reverse_opinion_label)[shuffle_ix]\n            spans_opinion_label_shuffle = []\n            for spans_opinion_label_split in spans_opinion_label:\n                spans_opinion_label_split_np = np.array(spans_opinion_label_split)[shuffle_ix]\n                spans_opinion_label_shuffle.append(spans_opinion_label_split_np.tolist())\n            spans_opinion_label = spans_opinion_label_shuffle\n            reverse_aspect_label_shuffle = []\n            for reverse_aspect_label_split in reverse_aspect_label:\n                reverse_aspect_label_split_np = np.array(reverse_aspect_label_split)[shuffle_ix]\n                reverse_aspect_label_shuffle.append(reverse_aspect_label_split_np.tolist())\n            reverse_aspect_label = reverse_aspect_label_shuffle\n            spans, span_tokens, spans_aspect_label, reverse_opinion_label  = spans_np.tolist(), span_tokens_np.tolist(),\\\n                                                                             spans_aspect_label_np.tolist(), reverse_opinion_label_np.tolist()\n        related_spans = np.zeros((len(spans), len(spans)), dtype=int)\n        for i in range(len(span_tokens)):\n            span_token = span_tokens[i].split(' ')\n            # for j in range(i, len(span_tokens)):\n            for j in range(len(span_tokens)):\n                differ_span_token = span_tokens[j].split(' ')\n                if set(span_token) & set(differ_span_token) == set():\n                    related_spans[i, j] = 0\n                else:\n                    related_spans[i, j] = 1\n\n        sample['related_span_array'] = related_spans\n        sample['spans'] = spans\n        sample['span tokens'] = span_tokens\n        sample['spans_aspect_label'] = spans_aspect_label\n        sample['spans_opinion_label'] = spans_opinion_label\n        sample['reverse_opinion_label'] = reverse_opinion_label\n        sample['reverse_aspect_label'] = reverse_aspect_label\n        features.append(sample)\n    return features, num_aspect, num_opinion\n\n\nclass MyDataset:\n    def __init__(self, args, path, if_train=False):\n        self.args = args\n        with open(path, 'r', encoding='utf-8') as f:\n            lines = f.readlines()\n            if if_train:\n                random.seed(args.RANDOM_SEED)\n                random.shuffle(lines)\n            self.instances = load_data_instances_txt(lines)\n            self.data_instances, _, _ = convert_examples_to_features(\n                self.args, \n                train_instances=self.instances,\n                max_span_length=self.args.max_span_length)\n\n    def __len__(self):\n        return len(self.instances)\n\n    def __getitem__(self, idx):\n        return self.data_instances[idx]\n\n\n\ndef load_data_instances_txt(lines):\n    sentiment2sentiment = {'NEG': 'negative', 'POS': 'positive', 'NEU': 'neutral', 'STR': 'start'}\n\n    instances = list()\n    triples_num = 0\n    aspects_num = 0\n    for ex_index, line in enumerate(lines):\n        id = str(ex_index)  # id\n        line = line.strip()\n        line = line.split('####')\n        sentence = line[0].split()  # sentence\n        raw_pairs = eval(line[1])  # triplets\n\n        triple_dict = {}\n        aspect_num = 0\n        for triple in raw_pairs:\n            raw_aspect = triple[0]\n            raw_opinion = triple[1]\n            sentiment = sentiment2sentiment[triple[2]]\n\n            if len(raw_aspect) == 1:\n                aspect_word = sentence[raw_aspect[0]]\n                raw_aspect = [raw_aspect[0], raw_aspect[0]]\n            else:\n                aspect_word = ' '.join(sentence[raw_aspect[0]: raw_aspect[-1] + 1])\n            aspect_label = {}\n            aspect_label[aspect_word] = [raw_aspect[0], raw_aspect[-1]]\n            aspect_num += len(aspect_label)\n\n            if len(raw_opinion) == 1:\n                opinion_word = sentence[raw_opinion[0]]\n                raw_opinion = [raw_opinion[0], raw_opinion[0]]\n            else:\n                opinion_word = ' '.join(sentence[raw_opinion[0]: raw_opinion[-1] + 1])\n            opinion_label = {}\n            opinion_label[opinion_word] = [raw_opinion[0], raw_opinion[-1]]\n\n            word = str(aspect_word) + '|' + str(opinion_word)\n            if word not in triple_dict:\n                triple_dict[word] = []\n                triple_dict[word] = ([raw_aspect[0], raw_aspect[-1]], [raw_opinion[0], raw_opinion[-1]], sentiment)\n            else:\n                print('Single sentence ' + id + ' The middle triplet reappears!')\n        examples = InputExample(id=id, text_a=line[0], text_b=None, all_label=triple_dict, aspect_num=aspect_num,\n                                triple_num=len(triple_dict))\n\n        instances.append(examples)\n        triples_num += triples_num\n        aspects_num += aspect_num\n\n    return instances\n\n\nclass DataTterator(object):\n    def __init__(self, dataset, args):\n        self.dataset = dataset\n        # self.dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.train_batch_size)\n        self.args = args\n        self.batch_size = args.train_batch_size\n        self.batch_count = (len(dataset) - 1) // self.batch_size + 1\n        self.tokenizer = BertTokenizer.from_pretrained(args.init_vocab, do_lower_case=args.do_lower_case)\n\n    def get_instances(self, batch_num):\n        bb = batch_num * self.batch_size\n        instances = [self.dataset.__getitem__(i) for i in range(bb, min(bb + self.batch_size, len(self.dataset)))]\n        return instances\n\n    def get_batch(self, batch_num):\n        tokens_tensor_list = []\n        bert_spans_tensor_list = []\n        spans_ner_label_tensor_list = []\n        spans_aspect_tensor_list = []\n        spans_opinion_label_tensor_list = []\n\n        reverse_ner_label_tensor_list = []\n        reverse_opinion_tensor_list = []\n        reverse_aspect_tensor_list = []\n        sentence_length = []\n        related_spans_list = []\n        adj_batch = []\n\n        self.instances = self.get_instances(batch_num)\n        \n        max_tokens = self.args.max_seq_length\n        max_spans = 0\n        for i, sample in enumerate(self.instances):\n            tokens = sample['tokens']\n            spans = sample['spans']\n            span_tokens = sample['span tokens']\n            spans_ner_label = sample['spans_aspect_label']\n            spans_aspect2opinion_labels = sample['spans_aspect2opinion_label']\n            spans_opinion_label = sample['spans_opinion_label']\n\n            reverse_ner_label = sample['reverse_opinion_label']\n            reverse_opinion2aspect_labels = sample['reverse_opinion2aspect_label']\n            reverse_aspect_label = sample['reverse_aspect_label']\n            \n            related_spans = sample['related_span_array']\n            spans_aspect_labels, reverse_opinion_labels = [], []\n            for spans_aspect2opinion_label in spans_aspect2opinion_labels:\n                spans_aspect_labels.append((i, spans_aspect2opinion_label[0], spans_aspect2opinion_label[1]))\n            for reverse_opinion2aspect_label in reverse_opinion2aspect_labels:\n                reverse_opinion_labels.append((i, reverse_opinion2aspect_label[0], reverse_opinion2aspect_label[1]))\n            bert_tokens, tokens_tensor, bert_spans_tensor, spans_ner_label_tensor, spans_aspect_labels_tensor, spans_opinion_tensor, \\\n            reverse_ner_label_tensor, reverse_opinion_tensor, reverse_aspect_tensor = \\\n                self.get_input_tensors(self.tokenizer, tokens, spans, spans_ner_label, spans_aspect_labels,\n                                         spans_opinion_label, reverse_ner_label, reverse_opinion_labels, reverse_aspect_label)\n            tokens_tensor_list.append(tokens_tensor)\n            bert_spans_tensor_list.append(bert_spans_tensor)\n            spans_ner_label_tensor_list.append(spans_ner_label_tensor)\n            spans_aspect_tensor_list.append(spans_aspect_labels_tensor)\n            spans_opinion_label_tensor_list.append(spans_opinion_tensor)\n            reverse_ner_label_tensor_list.append(reverse_ner_label_tensor)\n            reverse_opinion_tensor_list.append(reverse_opinion_tensor)\n            reverse_aspect_tensor_list.append(reverse_aspect_tensor)\n            assert bert_spans_tensor.shape[1] == spans_ner_label_tensor.shape[1] == reverse_ner_label_tensor.shape[1]\n            # tokens和spans的最大个数被设定为固定值\n            if (tokens_tensor.shape[1] > max_tokens):\n                max_tokens = tokens_tensor.shape[1]\n            if (bert_spans_tensor.shape[1] > max_spans):\n                max_spans = bert_spans_tensor.shape[1]\n            sentence_length.append((bert_tokens, tokens_tensor.shape[1], bert_spans_tensor.shape[1]))\n            related_spans_list.append(related_spans)\n        \n        '''由于不同句子方阵不一样大，所以先不转为tensor'''\n        #related_spans_tensor = torch.tensor(related_spans_list)\n        # apply padding and concatenate tensors\n        final_tokens_tensor = []\n        final_attention_mask = []\n        final_spans_mask_tensor = []\n        final_bert_spans_tensor = []\n        final_spans_ner_label_tensor = []\n        final_spans_aspect_tensor = []\n        final_spans_opinion_label_tensor = []\n\n        final_reverse_ner_label_tensor = []\n        final_reverse_opinion_tensor = []\n        final_reverse_aspect_label_tensor = []\n        final_related_spans_tensor = []\n        for tokens_tensor, bert_spans_tensor, spans_ner_label_tensor, spans_aspect_tensor, spans_opinion_label_tensor, \\\n            reverse_ner_label_tensor, reverse_opinion_tensor, reverse_aspect_tensor, related_spans \\\n                in zip(tokens_tensor_list, bert_spans_tensor_list, spans_ner_label_tensor_list, spans_aspect_tensor_list,\n                       spans_opinion_label_tensor_list, reverse_ner_label_tensor_list, reverse_opinion_tensor_list,\n                       reverse_aspect_tensor_list, related_spans_list):\n            # padding for tokens\n            num_tokens = tokens_tensor.shape[1]\n            tokens_pad_length = max_tokens - num_tokens\n            attention_tensor = torch.full([1, num_tokens], 1, dtype=torch.long)\n            if tokens_pad_length > 0:\n                pad = torch.full([1, tokens_pad_length], self.tokenizer.pad_token_id, dtype=torch.long)\n                tokens_tensor = torch.cat((tokens_tensor, pad), dim=1)\n                attention_pad = torch.full([1, tokens_pad_length], 0, dtype=torch.long)\n                attention_tensor = torch.cat((attention_tensor, attention_pad), dim=1)\n\n            # padding for spans\n            num_spans = bert_spans_tensor.shape[1]\n            num_aspect = spans_aspect_tensor.shape[1]\n            num_opinion = reverse_opinion_tensor.shape[1]\n            spans_pad_length = max_spans - num_spans\n            spans_mask_tensor = torch.full([1, num_spans], 1, dtype=torch.long)\n            if spans_pad_length > 0:\n                pad = torch.full([1, spans_pad_length, bert_spans_tensor.shape[2]], 0, dtype=torch.long)\n                bert_spans_tensor = torch.cat((bert_spans_tensor, pad), dim=1)\n\n                mask_pad = torch.full([1, spans_pad_length], 0, dtype=torch.long)\n                spans_mask_tensor = torch.cat((spans_mask_tensor, mask_pad), dim=1)\n                spans_ner_label_tensor = torch.cat((spans_ner_label_tensor, mask_pad), dim=1)\n                reverse_ner_label_tensor = torch.cat((reverse_ner_label_tensor, mask_pad), dim=1)\n\n                opinion_mask_pad = torch.full([1, num_aspect, spans_pad_length], 0, dtype=torch.long)\n                spans_opinion_label_tensor = torch.cat((spans_opinion_label_tensor, opinion_mask_pad), dim=-1)\n                aspect_mask_pad = torch.full([1, num_opinion, spans_pad_length], 0, dtype=torch.long)\n                reverse_aspect_tensor = torch.cat((reverse_aspect_tensor, aspect_mask_pad), dim=-1)\n                '''对span类似方阵mask'''\n                related_spans = np.pad(related_spans, [(0, spans_pad_length), (0, spans_pad_length)])\n            related_spans_tensor = torch.as_tensor(torch.from_numpy(related_spans), dtype=torch.bool)\n            # update final outputs\n            \n            final_bert_spans_tensor.append(bert_spans_tensor)\n            final_tokens_tensor.append(tokens_tensor)\n            final_attention_mask.append(attention_tensor)\n            final_spans_mask_tensor.append(spans_mask_tensor)\n            final_spans_ner_label_tensor.append(spans_ner_label_tensor)\n            final_spans_aspect_tensor.append(spans_aspect_tensor.squeeze(0))\n            final_spans_opinion_label_tensor.append(spans_opinion_label_tensor.squeeze(0))\n            final_reverse_ner_label_tensor.append(reverse_ner_label_tensor)\n            final_reverse_opinion_tensor.append(reverse_opinion_tensor.squeeze(0))\n            final_reverse_aspect_label_tensor.append(reverse_aspect_tensor.squeeze(0))\n            final_related_spans_tensor.append(related_spans_tensor.unsqueeze(0))\n            \n\n        # 注意，特征中最大span间隔不一定为设置的max_span_length，这是因为bert分词之后造成的span扩大了。\n        final_tokens_tensor = torch.cat(final_tokens_tensor, dim=0).to(self.args.device)\n        final_attention_mask = torch.cat(final_attention_mask, dim=0).to(self.args.device)\n        final_bert_spans_tensor = torch.cat(final_bert_spans_tensor, dim=0).to(self.args.device)\n        final_spans_mask_tensor = torch.cat(final_spans_mask_tensor, dim=0).to(self.args.device)\n        final_spans_ner_label_tensor = torch.cat(final_spans_ner_label_tensor, dim=0).to(self.args.device)\n        final_spans_aspect_tensor = torch.cat(final_spans_aspect_tensor, dim=0).to(self.args.device)\n        final_spans_opinion_label_tensor = torch.cat(final_spans_opinion_label_tensor, dim=0).to(self.args.device)\n        final_reverse_ner_label_tensor = torch.cat(final_reverse_ner_label_tensor, dim=0).to(self.args.device)\n        final_reverse_opinion_tensor = torch.cat(final_reverse_opinion_tensor, dim=0).to(self.args.device)\n        final_reverse_aspect_label_tensor = torch.cat(final_reverse_aspect_label_tensor, dim=0).to(self.args.device)\n        final_related_spans_tensor = torch.cat(final_related_spans_tensor, dim=0).to(self.args.device)\n#         adj_matrix = torch.cat([i.unsqueeze(0) for i in adj_batch], axis=0)\n        return final_tokens_tensor, final_attention_mask, final_bert_spans_tensor, final_spans_mask_tensor, \\\n               final_spans_ner_label_tensor, final_spans_aspect_tensor, final_spans_opinion_label_tensor, \\\n               final_reverse_ner_label_tensor, final_reverse_opinion_tensor, final_reverse_aspect_label_tensor, \\\n               final_related_spans_tensor, sentence_length\n\n\n    def get_input_tensors(self, tokenizer, tokens, spans, spans_ner_label, spans_aspect_label, spans_opinion_label,\n                          reverse_ner_label, reverse_opinion_labels, reverse_aspect_label):\n        start2idx = []\n        end2idx = []\n        bert_tokens = []\n        bert_tokens.append(tokenizer.cls_token)\n        for token in tokens:\n            start2idx.append(len(bert_tokens))\n            test_1 = len(bert_tokens)\n            sub_tokens = tokenizer.tokenize(token)\n            if self.args.span_generation == \"CNN\":\n                bert_tokens.append(sub_tokens[0])\n            elif self.args.Only_token_head:\n                bert_tokens.append(sub_tokens[0])\n            else:\n                bert_tokens += sub_tokens\n            end2idx.append(len(bert_tokens) - 1)\n            test_2 = len(bert_tokens) - 1\n\n        bert_tokens.append(tokenizer.sep_token)\n        indexed_tokens = tokenizer.convert_tokens_to_ids(bert_tokens)\n        tokens_tensor = torch.tensor([indexed_tokens])\n        bert_spans = [[start2idx[span[0]], end2idx[span[1]], span[2]] for span in spans]\n        # 在bert分出subword之后  需要对原有的aspect span进行补充\n        spans_aspect_label = [[aspect_span[0], start2idx[aspect_span[1]], end2idx[aspect_span[2]]] for\n                              aspect_span in spans_aspect_label]\n        reverse_opinion_label =[[opinion_span[0], start2idx[opinion_span[1]], end2idx[opinion_span[2]]] for\n                                opinion_span in reverse_opinion_labels]\n        bert_spans_tensor = torch.tensor([bert_spans])\n\n        spans_ner_label_tensor = torch.tensor([spans_ner_label])\n        spans_aspect_tensor = torch.tensor([spans_aspect_label])\n        spans_opinion_tensor = torch.tensor([spans_opinion_label])\n        reverse_ner_label_tensor = torch.tensor([reverse_ner_label])\n        reverse_opinion_tensor = torch.tensor([reverse_opinion_label])\n        reverse_aspect_tensor = torch.tensor([reverse_aspect_label])\n        return bert_tokens, tokens_tensor, bert_spans_tensor, spans_ner_label_tensor, spans_aspect_tensor, spans_opinion_tensor, \\\n               reverse_ner_label_tensor, reverse_opinion_tensor, reverse_aspect_tensor\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T09:24:45.183263Z","iopub.execute_input":"2024-04-30T09:24:45.183621Z","iopub.status.idle":"2024-04-30T09:24:45.217081Z","shell.execute_reply.started":"2024-04-30T09:24:45.183596Z","shell.execute_reply":"2024-04-30T09:24:45.216170Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Writing data_BIO_loader.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile eval_features.py\n\ndef unbatch_data(pred_data):\n    stage1_pred = []\n    stage1_pred_sentiment = []\n    stage1_pred_sentiment_logits = []\n    stage2_pred = []\n    stage2_pred_sentiment_logits = []\n\n    for i in range(len(pred_data[0])):\n        pred_stage1_result_tolist = pred_data[0][i].tolist()\n        pred_stage1_result_sentiment_tolist = pred_data[1][i].tolist()\n        pred_stage1_sentiment_logit_tolist = pred_data[2][i].tolist()\n\n        pred_stage2_result_tolist = pred_data[3][i].tolist()\n        pred_stage2_sentiment_logit_tolist = pred_data[4][i].tolist()\n\n        # test\n        if len(pred_stage1_result_tolist) != len(pred_stage2_result_tolist):\n            raise IndexError('预测的stage1和stage2序列数不相等')\n        for j in range(len(pred_stage1_result_sentiment_tolist)):\n            pred_stage1_per_sent, pred_stage2_per_sent, pred_stage2_sentiment_logit_per_sent = [], [], []\n\n            stage1_pred_sentiment.append(pred_stage1_result_sentiment_tolist[j])\n            stage1_pred_sentiment_logits.append(pred_stage1_sentiment_logit_tolist[j])\n\n            for k2, pred_span in enumerate(pred_stage1_result_tolist):\n                if pred_span[0] == j:\n                    pred_stage1_per_sent.append(pred_span)\n                    pred_stage2_per_sent.append(pred_stage2_result_tolist[k2])\n                    pred_stage2_sentiment_logit_per_sent.append(pred_stage2_sentiment_logit_tolist[k2])\n\n            stage1_pred.append(pred_stage1_per_sent)\n            stage2_pred.append(pred_stage2_per_sent)\n            stage2_pred_sentiment_logits.append(pred_stage2_sentiment_logit_per_sent)\n\n    pred_result = (stage1_pred, stage1_pred_sentiment, stage1_pred_sentiment_logits, stage2_pred,\n                   stage2_pred_sentiment_logits)\n    return pred_result","metadata":{"execution":{"iopub.status.busy":"2024-04-30T09:24:45.218130Z","iopub.execute_input":"2024-04-30T09:24:45.218401Z","iopub.status.idle":"2024-04-30T09:24:45.235639Z","shell.execute_reply.started":"2024-04-30T09:24:45.218379Z","shell.execute_reply":"2024-04-30T09:24:45.234773Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Writing eval_features.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile Metric.py\n\nfrom transformers import BertTokenizer\nimport numpy as np\nimport json\nfrom tqdm import tqdm\n\n\nid4validity = {0: 'none', 1: 'valid'}\nid4sentiment = {0: 'none', 1: 'positive', 2: 'negative', 3:'neutral', 4:'start'}\n\n\nclass Metric():\n    def __init__(self, args, forward_pred_result, reverse_pred_result, gold_instances):\n        self.args = args\n        self.gold_instances = gold_instances\n        self.tokenizer = BertTokenizer.from_pretrained(args.init_vocab, do_lower_case=args.do_lower_case)\n\n        self.pred_aspect = forward_pred_result[0]\n        self.pred_aspect_sentiment = forward_pred_result[1]\n        self.pred_aspect_sentiment_logit = forward_pred_result[2]\n\n        self.pred_opinion = forward_pred_result[3]\n        self.pred_opinion_sentiment_logit = forward_pred_result[4]\n\n        '''Reverse evaluation'''\n        self.reverse_pred_opinon = reverse_pred_result[0]\n        self.reverse_pred_opinon_sentiment = reverse_pred_result[1]\n        self.reverse_pred_opinon_sentiment_logit = reverse_pred_result[2]\n\n        self.reverse_pred_aspect = reverse_pred_result[3]\n        self.reverse_pred_aspect_sentiment_logit = reverse_pred_result[4]\n\n\n    def P_R_F1(self, gold_num, pred_num, correct_num):\n        precision = correct_num / pred_num if pred_num > 0 else 0\n        recall = correct_num / gold_num if gold_num > 0 else 0\n        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n        return (precision, recall, f1)\n\n    def num_4_eval(self, gold, pred, gold_num, pred_num, correct_num):\n        correct = set(gold) & set(pred)\n        gold_num += len(set(gold))\n        pred_num += len(set(pred))\n        correct_num += len(correct)\n        return gold_num, pred_num, correct_num\n\n    def cal_triplet_final_result(self, forward_results, forward_spans, reverse_results, reverse_spans):\n\n        pred_dicts = {}\n        pred_spans = forward_spans + reverse_spans\n        for index, result in enumerate(forward_results + reverse_results):\n            if result in pred_dicts:\n                score_dict = pred_dicts[result][2]\n                score_new = pred_spans[index][2]\n                if score_dict > score_new:\n                    continue\n                else:\n                    pred_dicts[result] = pred_spans[index]\n            else:\n                pred_dicts[result] = pred_spans[index]\n        history = []\n        for i in pred_dicts:\n            aspect_span_i = range(pred_dicts[i][0][0], pred_dicts[i][0][1])\n            opinion_span_i = range(pred_dicts[i][1][0], pred_dicts[i][1][1])\n            for j in pred_dicts:\n                if (i,j) in history:\n                    continue\n                history.append((i, j))\n                history.append((j, i))\n                if i == j:\n                    continue\n                aspect_span_j = range(pred_dicts[j][0][0], pred_dicts[j][0][1])\n                opinion_span_j = range(pred_dicts[j][1][0], pred_dicts[j][1][1])\n                repeat_a_span = list(set(aspect_span_i) & set(aspect_span_j))\n                repeat_o_span = list(set(opinion_span_i) & set(opinion_span_j))\n                if len(repeat_a_span) == 0 or len(repeat_o_span) == 0:\n                    continue\n                elif len(repeat_a_span) <= min(len(aspect_span_i), len(aspect_span_j)) and \\\n                        len(repeat_o_span) <= min(len(opinion_span_i), len(opinion_span_j)):\n                    i_score = pred_dicts[i][2]\n                    j_score = pred_dicts[j][2]\n                    if i_score >= j_score:\n                        pred_dicts[j] = (pred_dicts[j][0], pred_dicts[j][1], 0)\n                    else:\n                        pred_dicts[i] = (pred_dicts[i][0], pred_dicts[i][1], 0)\n                else:\n                    raise(KeyboardInterrupt)\n        return [_ for _ in pred_dicts if pred_dicts[_][2] != 0]\n\n\n    def score_triples(self):\n        correct_aspect_num,correct_opinion_num,correct_apce_num,correct_pairs_num,correct_num = 0,0,0,0,0\n        gold_aspect_num,gold_opinion_num,gold_apce_num,gold_pairs_num,gold_num = 0,0,0,0,0\n        pred_aspect_num,pred_opinion_num,pred_apce_num,pred_pairs_num,pred_num = 0,0,0,0,0\n\n        if self.args.output_path:\n            result = []\n            aspect_text = []\n            opinion_text = []\n        print(\"Ready to for\")\n        for i in tqdm(range(len(self.gold_instances))):\n            '''Entity length experiment'''\n\n            bert_tokens = []\n            spans = self.gold_instances[i]['spans']\n            start2idx = []\n            end2idx = []\n            bert_tokens.append(self.tokenizer.cls_token)\n            for token in self.gold_instances[i]['tokens']:\n                start2idx.append(len(bert_tokens))\n                sub_tokens = self.tokenizer.tokenize(token)\n                if self.args.span_generation == \"CNN\":\n                    bert_tokens.append(sub_tokens[0])\n                elif self.args.Only_token_head:\n                    bert_tokens.append(sub_tokens[0])\n                else:\n                    bert_tokens += sub_tokens\n                end2idx.append(len(bert_tokens) - 1)\n            bert_tokens.append(self.tokenizer.sep_token)\n            bert_spans = [[start2idx[span[0]], end2idx[span[1]], span[2]] for span in spans]\n            gold_aspect, gold_opinion, gold_apce, gold_pairs, gold_triples = self.find_gold_triples(i, bert_spans,\n                                                                                                    bert_tokens)\n            pred_aspect, pred_opinion, pred_apce, pred_pairs, pred_triples, pred_spans = self.find_pred_triples(i, bert_spans,\n                                                                                                    bert_tokens)\n\n            # if len(gold_triples) < 5:\n            #     continue\n\n            reverse_aspect, reverse_opinion, reverse_apce, reverse_pairs, reverse_triples, reverse_spans = \\\n                self.find_pred_reverse_triples(i, bert_spans, bert_tokens)\n\n            pred_aspect = list(set(pred_aspect) | set(reverse_aspect))\n            pred_opinion = list(set(pred_opinion) | set(reverse_opinion))\n            pred_apce = list(set(pred_apce) | set(reverse_apce))\n            pred_pairs = list(set(pred_pairs) | set(reverse_pairs))\n            if self.args.Filter_Strategy:\n                pred_triples = self.cal_triplet_final_result(pred_triples, pred_spans, reverse_triples, reverse_spans)\n            else:\n                pred_triples = list(set(pred_triples) | set(reverse_triples))\n\n\n            if self.args.output_path:\n                result.append({\"sentence\": self.gold_instances[i]['sentence'],\n                                     \"triple_list_gold\": [gold_triple for gold_triple in set(gold_triples)],\n                                     \"triple_list_pred\": [pred_triple for pred_triple in set(pred_triples)],\n                                    \"new\": [new_triple for new_triple in (set(pred_triples) - set(gold_triples))],\n                                    \"lack\": [lack_triple for lack_triple in (set(gold_triples) - set(pred_triples))]\n                                     })\n                aspect_text.append({\"sentence\": self.gold_instances[i]['sentence'],\n                                    'gold aspect': [gold_as for gold_as in set(gold_aspect)],\n                                    'pred aspect': [pred_as for pred_as in set(pred_aspect)],\n                                    \"new\": [new_as for new_as in (set(pred_aspect) - set(gold_aspect))],\n                                    \"lack\": [lack_as for lack_as in (set(gold_aspect) - set(pred_aspect))]})\n                opinion_text.append({\"sentence\": self.gold_instances[i]['sentence'],\n                                    'gold aspect': [gold_op for gold_op in set(gold_opinion)],\n                                    'pred aspect': [pred_op for pred_op in set(pred_opinion)],\n                                    \"new\": [new_op for new_op in (set(pred_opinion) - set(gold_opinion))],\n                                    \"lack\": [lack_op for lack_op in (set(gold_opinion) - set(pred_opinion))]})\n\n\n            gold_aspect_num, pred_aspect_num, correct_aspect_num = self.num_4_eval(gold_aspect, pred_aspect,\n                                                                                   gold_aspect_num,\n                                                                                   pred_aspect_num, correct_aspect_num)\n\n            gold_opinion_num, pred_opinion_num, correct_opinion_num = self.num_4_eval(gold_opinion, pred_opinion,\n                                                                                   gold_opinion_num,\n                                                                                   pred_opinion_num, correct_opinion_num)\n\n            gold_apce_num, pred_apce_num, correct_apce_num = self.num_4_eval(gold_apce, pred_apce, gold_apce_num,\n                                                                             pred_apce_num, correct_apce_num)\n\n            gold_apce_num, pred_apce_num, correct_apce_num = self.num_4_eval(gold_apce, pred_apce, gold_apce_num,\n                                                                             pred_apce_num, correct_apce_num)\n\n            gold_pairs_num, pred_pairs_num, correct_pairs_num = self.num_4_eval(gold_pairs, pred_pairs, gold_pairs_num,\n                                                                             pred_pairs_num, correct_pairs_num)\n\n            gold_num, pred_num, correct_num = self.num_4_eval(gold_triples, pred_triples, gold_num,\n                                                                                pred_num, correct_num)\n\n\n        if self.args.output_path:\n            F = open(self.args.dataset + 'triples.json', 'w', encoding='utf-8')\n            json.dump(result, F, ensure_ascii=False, indent=4)\n            F.close()\n\n            F1 = open(self.args.dataset + 'aspect.json', 'w', encoding='utf-8')\n            json.dump(aspect_text, F1, ensure_ascii=False, indent=4)\n            F1.close()\n\n            F2 = open(self.args.dataset + 'opinion.json', 'w', encoding='utf-8')\n            json.dump(opinion_text, F2, ensure_ascii=False, indent=4)\n            F2.close()\n\n\n        aspect_result = self.P_R_F1(gold_aspect_num, pred_aspect_num, correct_aspect_num)\n        opinion_result = self.P_R_F1(gold_opinion_num, pred_opinion_num, correct_opinion_num)\n        apce_result = self.P_R_F1(gold_apce_num, pred_apce_num, correct_apce_num)\n        pair_result = self.P_R_F1(gold_pairs_num, pred_pairs_num, correct_pairs_num)\n        triplet_result = self.P_R_F1(gold_num, pred_num, correct_num)\n        return aspect_result, opinion_result, apce_result, pair_result, triplet_result\n\n    def find_token(self, bert_tokens, span):\n        bert_tokens_4_span = bert_tokens[span[1]:span[2]+1]\n        sub = ''\n        for i, tokens in enumerate(bert_tokens_4_span):\n            if i == 0:\n                sub = tokens\n            elif '##' in tokens:\n                sub = sub + tokens.lstrip(\"##\")\n            else:\n                sub = sub +\" \"+ tokens\n        return sub\n\n    def gold_token(self, tokens):\n        sub = ''\n        for i, token in enumerate(tokens):\n            if i == 0:\n                sub = token\n            elif '##' in token:\n                sub = sub + token.lstrip(\"##\")\n            else:\n                sub = sub +\" \"+ token\n        return sub\n\n    def find_aspect_sentiment(self, sentence_index, bert_spans, span, aspect_sentiment, aspect_sentiment_logit):\n        # span = [span[1], span[2], ]\n        bert_span_index = [i for i,x in enumerate(bert_spans) if span[1] == x[0] and span[2] == x[1]]\n        assert len(bert_span_index) == 1\n        bert_span_index = bert_span_index[0]\n        sentiment_index = aspect_sentiment[sentence_index][bert_span_index]\n        # sentiment = id4sentiment[aspect_sentiment[sentence_index][bert_span_index]]\n        sentiment = id4validity[aspect_sentiment[sentence_index][bert_span_index]]\n        sentiment_logit = aspect_sentiment_logit[sentence_index][bert_span_index][sentiment_index]\n        # all_sentiment_logit = sum(aspect_sentiment_logit[sentence_index][bert_span_index])\n        # sentiment_precent = sentiment_logit / all_sentiment_logit\n        # return sentiment, sentiment_precent\n\n        return sentiment, sentiment_logit\n\n    def find_opinion_sentiment(self, sentence_index, opinion_index, bert_spans, span, opinion_sentiment,\n                               opinion_sentiment_logit):\n        bert_span_index = [i for i, x in enumerate(bert_spans) if span[1] == x[0] and span[2] == x[1]]\n        assert len(bert_span_index) == 1\n        bert_span_index = bert_span_index[0]\n        sentiment_index = opinion_sentiment[sentence_index][opinion_index][bert_span_index]\n        sentiment = id4sentiment[opinion_sentiment[sentence_index][opinion_index][bert_span_index]]\n        sentiment_logit = opinion_sentiment_logit[sentence_index][opinion_index][bert_span_index][sentiment_index]\n        return sentiment, sentiment_logit\n\n    # Code that uses raw data\n    def find_gold_triples(self, sentence_index, bert_spans, bert_tokens):\n        triples_list,pair_list = [],[]\n        aspect_list,opinion_list,apce_list = [],[],[]\n        triples = self.gold_instances[sentence_index]['triples']\n        for keys in triples:\n            aspect, opinion = keys.split('|')\n            aspect_tokens = []\n            for aspect_token in aspect.split( ):\n                token = self.tokenizer.tokenize(aspect_token)\n                if self.args.span_generation == \"CNN\":\n                    aspect_tokens.append(token[0])\n                elif self.args.Only_token_head:\n                    aspect_tokens.append(token[0])\n                else:\n                    aspect_tokens += token\n            new_aspect = self.gold_token(aspect_tokens)\n\n            opinion_tokens = []\n            for opinion_token in opinion.split( ):\n                token = self.tokenizer.tokenize(opinion_token)\n                if self.args.span_generation == \"CNN\":\n                    opinion_tokens.append(token[0])\n                elif self.args.Only_token_head:\n                    opinion_tokens.append(token[0])\n                else:\n                    opinion_tokens += token\n            new_opinion = self.gold_token(opinion_tokens)\n\n            sentiment = triples[keys][2]\n\n            triples_list.append((new_aspect, new_opinion, sentiment.lower()))\n\n            aspect_list.append((new_aspect))\n            opinion_list.append((new_opinion))\n\n            apce_list.append((new_aspect, sentiment))\n            pair_list.append((new_aspect, new_opinion))\n        return aspect_list, opinion_list, apce_list, pair_list, triples_list\n\n    def find_pred_triples(self, sentence_index, bert_spans, bert_tokens):\n        triples_list, pair_list, span_list = [], [], []\n        aspect_list, pred_opinion_list, apce_list = [], [], []\n        pred_aspect_span = self.pred_aspect[sentence_index]\n        # Remove duplicate aspects\n        new_aspect_span = []\n        for i, pred_aspect in enumerate(pred_aspect_span):\n            if len(new_aspect_span) == 0:\n                new_aspect_span.append(pred_aspect)\n            else:\n                if pred_aspect[1] == new_aspect_span[-1][1]:\n                    new_aspect_span[-1] = pred_aspect\n                else:\n                    new_aspect_span.append(pred_aspect)\n        for j, pred_aspect in enumerate(new_aspect_span):\n            aspect = self.find_token(bert_tokens, pred_aspect)\n            aspect_span_output = [pred_aspect[1], pred_aspect[2]+1]\n            aspect_sentiment, aspect_sentiment_logit = self.find_aspect_sentiment(sentence_index, bert_spans,\n                                                                                  pred_aspect,\n                                                                                  self.pred_aspect_sentiment,\n                                                                                  self.pred_aspect_sentiment_logit)\n            aspect_list.append(aspect)\n\n            opinion_list = []\n            for opinion_index in list(np.where(np.array(self.pred_opinion[sentence_index][j]) != 0)[0]):\n                opinion_list.append(opinion_index)\n            opinion_spans = []\n            for opinion_index in opinion_list:\n                if opinion_index < len(bert_spans):\n                    opinion_spans.append(bert_spans[opinion_index])\n                else:\n                    continue\n            new_opinion_spans = []\n            for i, pred_opinion in enumerate(opinion_spans):\n                if len(new_opinion_spans) == 0:\n                    new_opinion_spans.append(pred_opinion)\n                else:\n                    if pred_opinion[1] == new_opinion_spans[-1][1]:\n                        new_opinion_spans[-1] = pred_opinion\n                    else:\n                        new_opinion_spans.append(pred_opinion)\n            for opinion_span in new_opinion_spans:\n                opinion_span = (opinion_span[2], opinion_span[0], opinion_span[1])\n                opinion_span_output = [opinion_span[1], opinion_span[2]+1]\n                opinion = self.find_token(bert_tokens, opinion_span)\n                opinion_sentiment, opinion_sentiment_logit = self.find_opinion_sentiment(sentence_index, j, bert_spans,\n                                                                                         opinion_span,\n                                                                                         self.pred_opinion,\n                                                                                         self.pred_opinion_sentiment_logit)\n                # 筛选情感  弃用\n                # if opinion_sentiment_logit > aspect_sentiment_logit:\n                #     sentiment = opinion_sentiment\n                # else:\n                #     sentiment = aspect_sentiment\n\n                pred_opinion_list.append(opinion)\n                apce_list.append((aspect, opinion_sentiment))\n                triples_list.append((aspect, opinion, opinion_sentiment))\n                pair_list.append((aspect, opinion))\n                span_list.append((aspect_span_output, opinion_span_output, opinion_sentiment_logit))\n        return aspect_list, pred_opinion_list, apce_list, pair_list, triples_list, span_list\n\n\n\n    def find_pred_reverse_triples(self, sentence_index, bert_spans, bert_tokens):\n        triples_list, pair_list, span_list = [], [], []\n        opinion_list, pred_aspect_list, apce_list = [], [], []\n        pred_opinion_span = self.reverse_pred_opinon[sentence_index]\n\n        new_opinion_span = []\n        for i, pred_opinion in enumerate(pred_opinion_span):\n            if len(new_opinion_span) == 0:\n                new_opinion_span.append(pred_opinion)\n            else:\n                '''Take the long operation, the overlapping entity takes the longer part'''\n                if pred_opinion[1] == new_opinion_span[-1][1]:\n                    new_opinion_span[-1] = pred_opinion\n                else:\n                    new_opinion_span.append(pred_opinion)\n        for j, pred_opinion in enumerate(new_opinion_span):\n            opinion = self.find_token(bert_tokens, pred_opinion)\n            opinion_span_output = [pred_opinion[1], pred_opinion[2] + 1]\n            opinion_sentiment, opinion_sentiment_precent = self.find_aspect_sentiment(sentence_index,\n                                                                                    bert_spans,\n                                                                                    pred_opinion,\n                                                                                    self.reverse_pred_opinon_sentiment,\n                                                                                    self.reverse_pred_opinon_sentiment_logit)\n            opinion_list.append((opinion))\n            aspect_list = []\n            for aspect_index in list(np.where(np.array(self.reverse_pred_aspect[sentence_index][j]) != 0)[0]):\n                aspect_list.append(aspect_index)\n            aspect_spans = []\n            for aspect_index in aspect_list:\n                if aspect_index < len(bert_spans):\n                    aspect_spans.append(bert_spans[aspect_index])\n                else: continue\n            new_aspect_spans = []\n            '''At the beginning of the same, choose a longer entity'''\n            for i, pred_aspect in enumerate(aspect_spans):\n                if len(new_aspect_spans) == 0:\n                    new_aspect_spans.append(pred_aspect)\n                else:\n\n                    if pred_aspect[1] == new_aspect_spans[-1][1]:\n                        new_aspect_spans[-1] = pred_aspect\n                    else:\n                        new_aspect_spans.append(pred_aspect)\n            for aspect_span in new_aspect_spans:\n                aspect_span = (aspect_span[2], aspect_span[0], aspect_span[1])\n                aspect_span_output = [aspect_span[1], aspect_span[2] + 1]\n                aspect = self.find_token(bert_tokens, aspect_span)\n                aspect_sentiment, aspect_sentiment_precent = self.find_opinion_sentiment(sentence_index, j,\n                                                                                       bert_spans, aspect_span,\n                                                                                       self.reverse_pred_aspect,\n                                                                                       self.reverse_pred_aspect_sentiment_logit)\n                # if opinion_sentiment_precent > aspect_sentiment_precent:\n                #     sentiment = opinion_sentiment\n                # else:\n                #     sentiment = aspect_sentiment\n                pred_aspect_list.append((aspect))\n                apce_list.append((aspect, aspect_sentiment))\n                triples_list.append((aspect, opinion, aspect_sentiment))\n                pair_list.append((aspect, opinion))\n                span_list.append((aspect_span_output, opinion_span_output, aspect_sentiment_precent))\n        return pred_aspect_list, opinion_list, apce_list, pair_list, triples_list, span_list\n\nif __name__ == '__main__':\n    test1 = ('boot time', 'fast', 'pos')\n    test = ('boot time', 'boot')\n    test2 = ('Boot time', 'fast', 'pos')\n    set1  = set(test1) & set(test2)\n    print(set(test))","metadata":{"execution":{"iopub.status.busy":"2024-04-30T09:24:45.237086Z","iopub.execute_input":"2024-04-30T09:24:45.237546Z","iopub.status.idle":"2024-04-30T09:24:45.262767Z","shell.execute_reply.started":"2024-04-30T09:24:45.237517Z","shell.execute_reply":"2024-04-30T09:24:45.261993Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Writing Metric.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile model.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# import numpy\n# from transformers.models.bert.modeling_bert import BertAttention, BertIntermediate, BertOutput\n\nfrom Attention import Attention, Intermediate, Output, Dim_Four_Attention, masked_softmax\nfrom data_BIO_loader import sentiment2id, validity2id\nfrom allennlp.nn.util import batched_index_select, batched_span_select\nimport random\nimport math\n\ndef stage_2_features_generation(bert_feature, attention_mask, spans, span_mask, spans_embedding, spans_aspect_tensor,\n                                spans_opinion_tensor=None):\n    # Process the input aspect information in reverse to remove invalid aspects span\n    all_span_aspect_tensor = None\n    all_span_opinion_tensor = None\n    all_bert_embedding = None\n    all_attention_mask = None\n    all_spans_embedding = None\n    all_span_mask = None\n    spans_aspect_tensor_spilt = torch.chunk(spans_aspect_tensor, spans_aspect_tensor.shape[0], dim=0)\n    for i, spans_aspect_tensor_unspilt in enumerate(spans_aspect_tensor_spilt):\n        test = spans_aspect_tensor_unspilt.squeeze(0)\n        batch_num = spans_aspect_tensor_unspilt.squeeze(0)[0]\n        # mask4span_start = torch.where(span_mask[batch_num, :] == 1, spans[batch_num, :, 0], torch.tensor(-1).type_as(spans))\n        span_index_start = torch.where(spans[batch_num, :, 0] == spans_aspect_tensor_unspilt.squeeze()[1],\n                                       spans[batch_num, :, 1], torch.tensor(-1).type_as(spans))\n        span_index_end = torch.where(span_index_start == spans_aspect_tensor_unspilt.squeeze()[2], span_index_start,\n                                     torch.tensor(-1).type_as(spans))\n        span_index = torch.nonzero((span_index_end > -1), as_tuple=False).squeeze(0)\n        if min(span_index.shape) == 0:\n            continue\n        if spans_opinion_tensor is not None:\n            spans_opinion_tensor_unspilt = spans_opinion_tensor[i,:].unsqueeze(0)\n        aspect_span_embedding_unspilt = spans_embedding[batch_num, span_index, :].unsqueeze(0)\n        bert_feature_unspilt = bert_feature[batch_num, :, :].unsqueeze(0)\n        attention_mask_unspilt = attention_mask[batch_num, :].unsqueeze(0)\n        spans_embedding_unspilt = spans_embedding[batch_num, :, :].unsqueeze(0)\n        span_mask_unspilt = span_mask[batch_num, :].unsqueeze(0)\n        if all_span_aspect_tensor is None:\n            if spans_opinion_tensor is not None:\n                all_span_opinion_tensor = spans_opinion_tensor_unspilt\n            all_span_aspect_tensor = aspect_span_embedding_unspilt\n            all_bert_embedding = bert_feature_unspilt\n            all_attention_mask = attention_mask_unspilt\n            all_spans_embedding = spans_embedding_unspilt\n            all_span_mask = span_mask_unspilt\n        else:\n            if spans_opinion_tensor is not None:\n                all_span_opinion_tensor = torch.cat((all_span_opinion_tensor, spans_opinion_tensor_unspilt), dim=0)\n            all_span_aspect_tensor = torch.cat((all_span_aspect_tensor, aspect_span_embedding_unspilt), dim=0)\n            all_bert_embedding = torch.cat((all_bert_embedding, bert_feature_unspilt), dim=0)\n            all_attention_mask = torch.cat((all_attention_mask, attention_mask_unspilt), dim=0)\n            all_spans_embedding = torch.cat((all_spans_embedding, spans_embedding_unspilt), dim=0)\n            all_span_mask = torch.cat((all_span_mask, span_mask_unspilt), dim=0)\n    return all_span_opinion_tensor, all_span_aspect_tensor, all_bert_embedding, all_attention_mask, \\\n           all_spans_embedding, all_span_mask\n\n\nclass Step_1_module(torch.nn.Module):\n    def __init__(self, args, bert_config):\n        super(Step_1_module, self).__init__()\n        self.args = args\n        self.intermediate = Intermediate(bert_config)\n        self.output = Output(bert_config)\n\n    def forward(self, spans_embedding):\n        intermediate_output = self.intermediate(spans_embedding)\n        layer_output = self.output(intermediate_output, spans_embedding)\n        return layer_output, layer_output\n\n\nclass Step_1(torch.nn.Module):\n    def feature_slice(self, features, mask, span_mask, sentence_length):\n        cnn_span_generate_list = []\n        for j, CNN_generation_model in enumerate(self.CNN_span_generation):\n            bert_feature = features.permute(0, 2, 1)\n            cnn_result = CNN_generation_model(bert_feature)\n            cnn_span_generate_list.append(cnn_result)\n\n        features_sliced_tensor = None\n        features_mask_tensor = None\n        for i in range(features.shape[0]):\n            last_mask = torch.nonzero(mask[i, :])\n            features_sliced = features[i,:last_mask.shape[0]][1:-1]\n            for j in range(self.args.max_span_length -1):\n                if last_mask.shape[0] - 2 > j:\n                    # test = cnn_span_generate_list[j].permute(0, 2, 1)\n                    cnn_feature = cnn_span_generate_list[j].permute(0, 2, 1)[i, 1:last_mask.shape[0] - (j+2), :]\n                    features_sliced = torch.cat((features_sliced, cnn_feature), dim=0)\n                else:\n                    break\n            pad_length = span_mask.shape[1] - features_sliced.shape[0]\n            spans_mask_tensor = torch.full([1, features_sliced.shape[0]], 1, dtype=torch.long).to(self.args.device)\n            if pad_length > 0:\n                pad = torch.full([pad_length, self.args.bert_feature_dim], 0, dtype=torch.long).to(self.args.device)\n                features_sliced = torch.cat((features_sliced, pad),dim=0)\n                mask_pad = torch.full([1, pad_length], 0, dtype=torch.long).to(self.args.device)\n                spans_mask_tensor = torch.cat((spans_mask_tensor, mask_pad),dim=1)\n            if features_sliced_tensor is None:\n                features_sliced_tensor = features_sliced.unsqueeze(0)\n                features_mask_tensor = spans_mask_tensor\n            else:\n                features_sliced_tensor = torch.cat((features_sliced_tensor, features_sliced.unsqueeze(0)), dim=0).to(self.args.device)\n                features_mask_tensor = torch.cat((features_mask_tensor, spans_mask_tensor), dim=0).to(self.args.device)\n\n        return features_sliced_tensor, features_mask_tensor\n\n    def __init__(self, args, bert_config):\n        super(Step_1, self).__init__()\n        self.args = args\n        self.bert_config = bert_config\n        self.dropout_output = torch.nn.Dropout(args.drop_out)\n        if self.args.span_generation == \"Start_end\":\n            # 注意此处最大长度要加1的原因是在无效的span的mask由0表示  和其他的span长度结合在一起\n            self.step_1_embedding4width = nn.Embedding(args.max_span_length + 1, args.embedding_dim4width)\n            self.step_1_linear4width = nn.Linear(args.embedding_dim4width + args.bert_feature_dim * 2,\n                                                 args.bert_feature_dim)\n        elif self.args.span_generation == \"CNN\":\n            self.CNN_span_generation = nn.ModuleList(\n                [nn.Conv1d(in_channels=args.bert_feature_dim, out_channels=args.bert_feature_dim, kernel_size=i + 2) for\n                 i in range(args.max_span_length - 1)])\n        elif self.args.span_generation == \"ATT\":\n            self.ATT_attentions = nn.ModuleList(\n                [Dim_Four_Block(args, self.bert_config) for _ in range(max(1, args.ATT_SPAN_block_num - 1))])\n        elif self.args.span_generation == \"SE_ATT\":\n            self.compess_projection = nn.Sequential(nn.Linear(args.bert_feature_dim, 1), nn.ReLU(), nn.Dropout(args.drop_out))\n\n        if args.related_span_underline:\n            self.related_attentions = nn.ModuleList(\n                [Pointer_Block(args, self.bert_config) for _ in range(max(1, args.related_span_block_num - 1))])\n\n        self.forward_1_decoders = nn.ModuleList(\n            [Step_1_module(args, self.bert_config) for _ in range(max(1, args.block_num - 1))])\n        self.sentiment_classification_aspect = nn.Linear(args.bert_feature_dim, len(validity2id) - 2)\n        # self.sentiment_classification_aspect = nn.Linear(args.bert_feature_dim, len(sentiment2id))\n\n        self.reverse_1_decoders = nn.ModuleList(\n            [Step_1_module(args, self.bert_config) for _ in range(max(1, args.block_num - 1))])\n        self.sentiment_classification_opinion = nn.Linear(args.bert_feature_dim, len(validity2id) - 2)\n        # self.sentiment_classification_opinion = nn.Linear(args.bert_feature_dim, len(sentiment2id))\n\n    def forward(self, input_bert_features, attention_mask, spans, span_mask, related_spans_tensor, sentence_length):\n\n        spans_embedding, features_mask_tensor = self.span_generator(input_bert_features, attention_mask, spans,\n                                                                    span_mask, related_spans_tensor, sentence_length)\n\n        if self.args.related_span_underline:\n            # spans_embedding_0 = torch.clone(spans_embedding)\n            for related_attention in self.related_attentions:\n                related_layer_output, related_intermediate_output = related_attention(spans_embedding,\n                                                                                      related_spans_tensor,\n                                                                                      spans_embedding)\n                spans_embedding = related_layer_output\n            # spans_embedding = spans_embedding + spans_embedding_0\n\n        span_embedding_1 = torch.clone(spans_embedding)\n        for forward_1_decoder in self.forward_1_decoders:\n            forward_layer_output, forward_intermediate_output = forward_1_decoder(span_embedding_1)\n            span_embedding_1 = forward_layer_output\n        class_logits_aspect = self.sentiment_classification_aspect(span_embedding_1)\n\n        span_embedding_2 = torch.clone(spans_embedding)\n        for reverse_1_decoder in self.reverse_1_decoders:\n            reverse_layer_output, reverse_intermediate_output = reverse_1_decoder(span_embedding_2)\n            span_embedding_2 = reverse_layer_output\n        class_logits_opinion = self.sentiment_classification_opinion(span_embedding_2)\n\n        return class_logits_aspect, class_logits_opinion, spans_embedding, span_embedding_1, span_embedding_2, \\\n               features_mask_tensor\n\n    def span_generator(self, input_bert_features, attention_mask, spans, span_mask, related_spans_tensor,\n                       sentence_length):\n        bert_feature = self.dropout_output(input_bert_features)\n        features_mask_tensor = None\n        if self.args.span_generation == \"Average\" or self.args.span_generation == \"Max\":\n            # 如果使用全部span的bert信息：\n            spans_num = spans.shape[1]\n            spans_width_start_end = spans[:, :, 0:2].view(spans.size(0), spans_num, -1)\n            spans_width_start_end_embedding, spans_width_start_end_mask = batched_span_select(bert_feature,\n                                                                                              spans_width_start_end)\n            spans_width_start_end_mask = spans_width_start_end_mask.unsqueeze(-1).expand(-1, -1, -1,\n                                                                                         self.args.bert_feature_dim)\n            spans_width_start_end_embedding = torch.where(spans_width_start_end_mask, spans_width_start_end_embedding,\n                                                          torch.tensor(0).type_as(spans_width_start_end_embedding))\n            if self.args.span_generation == \"Max\":\n                spans_width_start_end_max = spans_width_start_end_embedding.max(2)\n                spans_embedding = spans_width_start_end_max[0]\n            else:\n                spans_width_start_end_mean = spans_width_start_end_embedding.mean(dim=2, keepdim=True).squeeze(-2)\n                spans_embedding = spans_width_start_end_mean\n        elif self.args.span_generation == \"Start_end\":\n            # 如果使用span区域大小进行embedding\n            spans_start = spans[:, :, 0].view(spans.size(0), -1)\n            spans_start_embedding = batched_index_select(bert_feature, spans_start)\n            spans_end = spans[:, :, 1].view(spans.size(0), -1)\n            spans_end_embedding = batched_index_select(bert_feature, spans_end)\n\n            spans_width = spans[:, :, 2].view(spans.size(0), -1)\n            spans_width_embedding = self.step_1_embedding4width(spans_width)\n            spans_embedding = torch.cat((spans_start_embedding, spans_width_embedding, spans_end_embedding), dim=-1)  # 预留可修改部分\n            # spans_embedding_dict = torch.cat((spans_start_embedding, spans_end_embedding, spans_width_embedding), dim=-1)\n            spans_embedding_dict = self.step_1_linear4width(spans_embedding)\n            spans_embedding = spans_embedding_dict\n        elif self.args.span_generation == \"CNN\":\n            feature_slice, features_mask_tensor = self.feature_slice(bert_feature, attention_mask, span_mask,\n                                                                     sentence_length)\n            spans_embedding = feature_slice\n        elif self.args.span_generation == \"ATT\":\n            spans_width_start_end = spans[:, :, 0:2].view(spans.shape[0], spans.shape[1], -1)\n            spans_width_start_end_embedding, spans_width_start_end_mask = batched_span_select(bert_feature,\n                                                                                              spans_width_start_end)\n            span_sum_embdding = torch.sum(spans_width_start_end_embedding, dim=2).unsqueeze(2)\n            for ATT_attention in self.ATT_attentions:\n                ATT_layer_output, ATT_intermediate_output = ATT_attention(span_sum_embdding,\n                                                                                      spans_width_start_end_mask,\n                                                                                      spans_width_start_end_embedding)\n                span_sum_embdding = ATT_layer_output\n            spans_embedding = span_sum_embdding.squeeze()\n        elif self.args.span_generation == \"SE_ATT\":\n            spans_width_start_end = spans[:, :, 0:2].view(spans.shape[0], spans.shape[1], -1)\n            spans_width_start_end_embedding, spans_width_start_end_mask = batched_span_select(bert_feature,\n                                                                                              spans_width_start_end)\n            spans_width_start_end_mask_2 = spans_width_start_end_mask.unsqueeze(-1).expand(-1, -1, -1,\n                                                                                         self.args.bert_feature_dim)\n            spans_width_start_end_embedding = torch.where(spans_width_start_end_mask_2, spans_width_start_end_embedding,\n                                                          torch.tensor(0).type_as(spans_width_start_end_embedding))\n            claim_self_att = self.compess_projection(spans_width_start_end_embedding).squeeze()\n            claim_self_att = torch.sum(spans_width_start_end_embedding, dim=-1).squeeze()\n            claim_rep = masked_softmax(claim_self_att, span_mask, spans_width_start_end_mask).unsqueeze(-1).transpose(2, 3)\n            claim_rep = torch.matmul(claim_rep, spans_width_start_end_embedding)\n            spans_embedding = claim_rep.squeeze()\n        return spans_embedding, features_mask_tensor\n\n\nclass Dim_Four_Block(torch.nn.Module):\n    def __init__(self, args, bert_config):\n        super(Dim_Four_Block, self).__init__()\n        self.args = args\n        self.forward_attn = Dim_Four_Attention(bert_config)\n        self.intermediate = Intermediate(bert_config)\n        self.output = Output(bert_config)\n        \n    def forward(self, hidden_embedding, masks, encoder_embedding):\n        #注意， mask需要和attention中的scores匹配，用来去掉对应的无意义的值\n        #对应的score的维度为 (batch_size, num_heads, hidden_dim, encoder_dim)\n        masks = (~masks) * -1e9\n        attention_masks = masks[:, :, None, None, :]\n        cross_attention_output = self.forward_attn(hidden_states=hidden_embedding,\n                                                   encoder_hidden_states=encoder_embedding,\n                                                   encoder_attention_mask=attention_masks)\n        attention_output = cross_attention_output[0]\n        attention_result = cross_attention_output[1:]\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output, attention_result\n\n\nclass Pointer_Block(torch.nn.Module):\n    def __init__(self, args, bert_config, mask_for_encoder=True):\n        super(Pointer_Block, self).__init__()\n        self.args = args\n        self.forward_attn = Attention(bert_config)\n        self.intermediate = Intermediate(bert_config)\n        self.output = Output(bert_config)\n        self.mask_for_encoder = mask_for_encoder\n\n    def forward(self, hidden_embedding, masks, encoder_embedding):\n        #Note that mask needs to match the scores in attention to remove the corresponding meaningless values\n        #The dimension of the corresponding score is (batch_size, num_heads, hidden_dim, encoder_dim)\n        masks = (~masks) * -1e9\n        if masks.dim() == 3:\n            attention_masks = masks[:, None, :, :]\n        elif masks.dim() == 2:\n            if self.mask_for_encoder:\n                attention_masks = masks[:, None, None, :]\n            else:\n                attention_masks = masks[:, None, :, None]\n        if self.mask_for_encoder:\n            cross_attention_output = self.forward_attn(hidden_states=hidden_embedding,\n                                                       encoder_hidden_states=encoder_embedding,\n                                                       encoder_attention_mask=attention_masks)\n        else:\n            cross_attention_output = self.forward_attn(hidden_states=hidden_embedding,\n                                                       encoder_hidden_states=encoder_embedding,\n                                                       attention_mask=attention_masks)\n        attention_output = cross_attention_output[0]\n        attention_result = cross_attention_output[1:]\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output, attention_result\n\n\nclass Step_2_forward(torch.nn.Module):\n    def __init__(self, args, bert_config):\n        super(Step_2_forward, self).__init__()\n        self.args = args\n        self.bert_config = bert_config\n        self.forward_opinion_decoder = nn.ModuleList(\n            [Pointer_Block(args, self.bert_config, mask_for_encoder=False) for _ in range(max(1, args.block_num - 1))])\n        self.opinion_docoder2class = nn.Linear(args.bert_feature_dim, len(sentiment2id))\n\n    def forward(self, aspect_spans_embedding, aspect_span_mask, spans_aspect_tensor):\n        '''aspect---> opinion direction'''\n        for opinion_decoder_layer in self.forward_opinion_decoder:\n            opinion_layer_output, opinion_attention = opinion_decoder_layer(aspect_spans_embedding, aspect_span_mask, spans_aspect_tensor)\n            aspect_spans_embedding = opinion_layer_output\n            # WHY ONLY THE LAST ONE???\n        opinion_class_logits = self.opinion_docoder2class(aspect_spans_embedding)\n        return opinion_class_logits, opinion_attention\n\n\nclass Step_2_reverse(torch.nn.Module):\n    def __init__(self, args, bert_config):\n        super(Step_2_reverse, self).__init__()\n        self.args = args\n        self.bert_config = bert_config\n        self.reverse_aspect_decoder = nn.ModuleList(\n            [Pointer_Block(args, self.bert_config, mask_for_encoder=False) for _ in range(max(1, args.block_num - 1))])\n        self.aspect_docoder2class = nn.Linear(args.bert_feature_dim, len(sentiment2id))\n\n    def forward(self, reverse_spans_embedding, reverse_span_mask, all_reverse_opinion_tensor):\n        '''opinion---> aspect direction'''\n        for reverse_aspect_decoder_layer in self.reverse_aspect_decoder:\n            aspect_layer_output, aspect_attention = reverse_aspect_decoder_layer(reverse_spans_embedding, reverse_span_mask, all_reverse_opinion_tensor)\n            reverse_spans_embedding = aspect_layer_output\n        aspect_class_logits = self.aspect_docoder2class(reverse_spans_embedding)\n        return aspect_class_logits, aspect_attention\n\n\n\ndef Loss(gold_aspect_label, pred_aspect_label, gold_opinion_label, pred_opinion_label, spans_mask_tensor, opinion_span_mask_tensor,\n         reverse_gold_opinion_label, reverse_pred_opinion_label, reverse_gold_aspect_label, reverse_pred_aspect_label,\n         cnn_spans_mask_tensor, reverse_aspect_span_mask_tensor, spans_embedding, related_spans_tensor, args):\n    loss_function = nn.CrossEntropyLoss(reduction='sum')\n    if cnn_spans_mask_tensor is not None:\n        spans_mask_tensor = cnn_spans_mask_tensor\n\n    # Loss Forward\n    aspect_spans_mask_tensor = spans_mask_tensor.view(-1) == 1\n    pred_aspect_label_logits = pred_aspect_label.view(-1, pred_aspect_label.shape[-1])\n    gold_aspect_effective_label = torch.where(aspect_spans_mask_tensor, gold_aspect_label.view(-1),\n                                              torch.tensor(loss_function.ignore_index).type_as(gold_aspect_label))\n    aspect_loss = loss_function(pred_aspect_label_logits, gold_aspect_effective_label)\n\n    opinion_span_mask_tensor = opinion_span_mask_tensor.view(-1) == 1\n    pred_opinion_label_logits = pred_opinion_label.view(-1, pred_opinion_label.shape[-1])\n    gold_opinion_effective_label = torch.where(opinion_span_mask_tensor, gold_opinion_label.view(-1),\n                                               torch.tensor(loss_function.ignore_index).type_as(gold_opinion_label))\n    opinion_loss = loss_function(pred_opinion_label_logits, gold_opinion_effective_label)\n    as_2_op_loss = aspect_loss + opinion_loss\n\n    # Loss Reverse direction\n    reverse_opinion_span_mask_tensor = spans_mask_tensor.view(-1) == 1\n    reverse_pred_opinion_label_logits = reverse_pred_opinion_label.view(-1, reverse_pred_opinion_label.shape[-1])\n    reverse_gold_opinion_effective_label = torch.where(reverse_opinion_span_mask_tensor, reverse_gold_opinion_label.view(-1),\n                                              torch.tensor(loss_function.ignore_index).type_as(reverse_gold_opinion_label))\n    reverse_opinion_loss = loss_function(reverse_pred_opinion_label_logits, reverse_gold_opinion_effective_label)\n\n    reverse_aspect_span_mask_tensor = reverse_aspect_span_mask_tensor.view(-1) == 1\n    reverse_pred_aspect_label_logits = reverse_pred_aspect_label.view(-1, reverse_pred_aspect_label.shape[-1])\n    reverse_gold_aspect_effective_label = torch.where(reverse_aspect_span_mask_tensor, reverse_gold_aspect_label.view(-1),\n                                               torch.tensor(loss_function.ignore_index).type_as(reverse_gold_aspect_label))\n    reverse_aspect_loss = loss_function(reverse_pred_aspect_label_logits, reverse_gold_aspect_effective_label)\n    op_2_as_loss = reverse_opinion_loss + reverse_aspect_loss\n\n    if args.kl_loss:\n        kl_loss = shape_span_embedding(args, spans_embedding, spans_embedding, related_spans_tensor, spans_mask_tensor)\n        # loss = as_2_op_loss + op_2_as_loss + kl_loss\n        loss = as_2_op_loss + op_2_as_loss + args.kl_loss_weight * kl_loss\n    else:\n        loss = as_2_op_loss + op_2_as_loss\n        kl_loss = 0\n    return loss, args.kl_loss_weight * kl_loss\n\ndef shape_span_embedding(args, p, q, pad_mask, span_mask):\n    kl_loss = 0\n    input_size = p.size()\n    assert input_size == q.size()\n    for i in range(input_size[0]):\n        span_mask_index = torch.nonzero(span_mask[i, :]).squeeze()\n        lucky_squence = random.choice(span_mask_index)\n        P = p[i, lucky_squence, :]\n        mask_index = torch.nonzero(pad_mask[i, lucky_squence, :])\n        q_tensor = None\n        for idx in mask_index:\n            if idx == lucky_squence:\n                continue\n            if q_tensor is None:\n                q_tensor = p[i, idx]\n            else:\n                q_tensor = torch.cat((q_tensor, p[i, idx]), dim=0)\n        if q_tensor is None:\n            continue\n        expan_P = P.expand_as(q_tensor)\n        kl_loss += compute_kl_loss(args, expan_P, q_tensor)\n    return kl_loss\n\ndef compute_kl_loss(args, p, q, pad_mask=None):\n    if args.kl_loss_mode == \"KLLoss\":\n        p_loss = F.kl_div(F.log_softmax(p, dim=-1), F.softmax(q, dim=-1), reduction=\"none\")\n        q_loss = F.kl_div(F.log_softmax(q, dim=-1), F.softmax(p, dim=-1), reduction=\"none\")\n\n        if pad_mask is not None:\n            p_loss.masked_fill(pad_mask, 0.)\n            q_loss.masked_fill(pad_mask, 0.)\n        p_loss = p_loss.sum()\n        q_loss = q_loss.sum()\n        total_loss = math.log(1+5/((p_loss + q_loss) / 2))\n    elif args.kl_loss_mode == \"JSLoss\":\n        m = (p+q)/2\n        m_loss = 0.5 * F.kl_div(F.log_softmax(p, dim=-1), F.softmax(m, dim=-1), reduction=\"none\") + 0.5 * F.kl_div(\n            F.log_softmax(q, dim=-1), F.softmax(m, dim=-1), reduction=\"none\")\n        if pad_mask is not None:\n            m_loss.masked_fill(pad_mask, 0.)\n        m_loss = m_loss.sum()\n        # test = -math.log(2*m_loss)-math.log(-2*m_loss+2)\n        total_loss = 10*(math.log(1+5/m_loss))\n    elif args.kl_loss_mode == \"EMLoss\":\n        test = torch.square(p-q)\n        em_loss = torch.sqrt(torch.sum(torch.square(p - q)))\n        total_loss = math.log(1+5/(em_loss))\n    elif args.kl_loss_mode == \"CSLoss\":\n        test = torch.cosine_similarity(p, q, dim=1)\n        cs_loss = torch.sum(torch.cosine_similarity(p, q, dim=1))\n        total_loss = math.log(1 + 5 / (cs_loss))\n    else:\n        total_loss = 0\n        print(\"what's wrong with you?\")\n    return  total_loss\n\n\nif __name__ == '__main__':\n#     tensor1 = torch.zeros((3,3))\n#     tensor2 = torch.nonzero(tensor1, as_tuple=False)\n#     tensor1 = tensor1.type_as(tensor2)\n    print('666')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T09:24:45.264033Z","iopub.execute_input":"2024-04-30T09:24:45.264346Z","iopub.status.idle":"2024-04-30T09:24:45.291681Z","shell.execute_reply.started":"2024-04-30T09:24:45.264322Z","shell.execute_reply":"2024-04-30T09:24:45.290798Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Writing model.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile gcn.py\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport spacy\nimport numpy as np\n\n# nlp = spacy.load(\"ru_core_news_md\")\n\nclass GCN(nn.Module):\n\n    def __init__(self, emb_dim=768, num_layers=1, gcn_dropout=0.7):             #此处dropout可以增大\n        super(GCN, self).__init__()\n        self.layers = num_layers\n        self.emb_dim = emb_dim\n        self.out_dim = emb_dim\n        input_dim = self.emb_dim\n        # gcn layer\n        self.W = nn.ModuleList([nn.Linear(input_dim, input_dim) for i in range(self.layers)])\n        self.gcn_drop = nn.Dropout(gcn_dropout)\n        self.relu = nn.ReLU()\n\n\n    def forward(self, adj, inputs, device):\n        # gcn layer\n\n        # adj (batch_size, len, len)\n        # inputs (batch_size, len, emb_dim)\n\n#         adj = adj.to_dense()\n        if inputs.shape[1] < adj.shape[1]:\n            adj = adj[:, :inputs.shape[1], :inputs.shape[1]]\n        \n        denom = adj.sum(2).unsqueeze(2) + 1                 # batch_size, len, 1\n#         mask = (adj.sum(2) + adj.sum(1)).eq(0).unsqueeze(2) # batch_size, len, 1\n\n        for layer in range(self.layers):\n            Ax = torch.bmm(adj, inputs)        # batch_size, len, emb_dim\n            AxW = self.W[layer](Ax)            # batch_size, len, emb_dim\n            AxW = AxW + self.W[layer](inputs)  # self loop\n            AxW = AxW.to(device) / denom\n            gAxW = self.relu(AxW)              # batch_size, len, emb_dim\n            if layer < self.layers - 1:\n                inputs = self.gcn_drop(gAxW)\n            else:\n                inputs = gAxW\n        return inputs, None # mask\n    \n\ndef make_adj_matrix(text, max_len=512):\n    doc = nlp(text)\n    doc = nlp(\" \".join(text))\n    adj_matrix = np.eye(max_len)\n    for (_, token) in enumerate(doc):\n        if token.i >= max_len or token.head.i >= max_len:\n            continue\n        adj_matrix[token.i][token.head.i] = 1\n    return torch.FloatTensor(adj_matrix).to_sparse()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T09:24:45.292868Z","iopub.execute_input":"2024-04-30T09:24:45.293176Z","iopub.status.idle":"2024-04-30T09:24:45.308856Z","shell.execute_reply.started":"2024-04-30T09:24:45.293143Z","shell.execute_reply":"2024-04-30T09:24:45.308029Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Writing gcn.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Run","metadata":{}},{"cell_type":"code","source":"%%writefile run.py\n\nimport os\nimport argparse\nimport tqdm\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AdamW, BertModel, get_linear_schedule_with_warmup\nfrom data_BIO_loader import DataTterator\nfrom data_BIO_loader import MyDataset\nfrom model import stage_2_features_generation, Step_1, Step_2_forward, Step_2_reverse, Loss\nfrom Metric import Metric\nfrom eval_features import unbatch_data\n# from kaggle.working.log import logger\nfrom thop import profile, clever_format\nimport wandb\nfrom transformers.models.bert.modeling_bert import BertEmbeddings\nfrom gcn import GCN, make_adj_matrix\nimport numpy as np\n\nimport logging\nfrom datetime import datetime\n\nimport time\nos.environ['CUDA_VISIBLE_DEVICES'] = '5'\n\n# import spacy\n# nlp = spacy.load(\"ru_core_news_md\")\n\nsentiment2id = {'none': 0, 'positive': 1, 'negative': 2, 'neutral': 3, 'start': 4}\n\nfrom datetime import datetime\n\nnow = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\nlogger = logging.getLogger(\"test\")\nlogger.setLevel(level=logging.INFO)\n\nhandler = logging.FileHandler(\"/kaggle/working/log/\"+now+\".log\", encoding='utf-8')\nhandler.setLevel(logging.INFO)\n\nconsole = logging.StreamHandler()\nconsole.setLevel(logging.INFO)\n\nlogger.addHandler(handler)\nlogger.addHandler(console)\n\n\nfrom natasha import (\n    Segmenter,\n    \n    NewsEmbedding,\n    NewsMorphTagger,\n    NewsSyntaxParser,\n    \n    Doc\n)\n\nsegmenter = Segmenter()\nemb = NewsEmbedding()\nmorph_tagger = NewsMorphTagger(emb)\nsyntax_parser = NewsSyntaxParser(emb)\n\n\ndef make_adj_matrix(sent, max_len):\n    new_sent = \"\"\n    new_inds = [] # from tokens to poses in text\n    new_inds_mapping = {}\n    index = -1\n    for idx, i in enumerate(sent[1:-1]):\n        if i[:2] == '##':\n            new_sent += i[2:]\n        else:\n            new_sent += \" \"\n            new_sent += i\n            index += 1\n        new_inds.append(index)\n\n    new_inds_mapping[0] = [0]\n    for idx, i in enumerate(new_inds):\n        if new_inds_mapping.get(i + 1):\n            new_inds_mapping[i + 1].append(idx + 1)\n        else:\n            new_inds_mapping[i + 1] = [idx + 1]\n\n    new_sent = new_sent.strip()\n    text = new_sent\n    \n    splitted_text = text.split(\" \")\n    \n    doc = Doc(text)\n    doc.segment(segmenter)\n    doc.tag_morph(morph_tagger)\n    doc.parse_syntax(syntax_parser)\n    \n    doc_sents_lens = [0]\n    for i in doc.sents:\n        doc_sents_lens.append(doc_sents_lens[-1] + len(i.tokens))\n    \n    cnt = 0\n    i = 0\n    j = 0\n    splitted_mapping = {0:[0]} # from poses from the text to segmented words\n    while i < len(doc.tokens) and j < len(splitted_text):\n        cur_nat_text = doc.tokens[i].text\n        cur_our_text = splitted_text[j]\n        if cur_nat_text == cur_our_text:\n            splitted_mapping[i + 1] = [j + 1]\n            i += 1\n            j += 1\n        else:\n            splitted_mapping[i + 1] = [j + 1]\n            if len(cur_nat_text) < len(cur_our_text):\n                while cur_nat_text != cur_our_text:\n                    i += 1\n                    splitted_mapping[i + 1] = [j + 1]\n\n                    cur_nat_text += doc.tokens[i].text\n            elif len(cur_nat_text) > len(cur_our_text):\n                while cur_nat_text != cur_our_text:\n                    j += 1\n                    splitted_mapping[i + 1].append(j + 1)\n                    cur_our_text += splitted_text[j]\n            else:\n                raise \"???\"\n            i += 1\n            j += 1\n\n#     adj_matrix = np.eye(max_len, max_len)\n    adj_matrix = np.zeros((max_len, max_len))\n\n    for i in doc.tokens:\n        sent_id, cur_id = [int(j)  for j in i.id.split('_')]\n        head_sent_id, head_id = [int(j) for j in i.head_id.split('_')]\n        cur_words_ids = []\n        for j in splitted_mapping[doc_sents_lens[sent_id - 1] + cur_id]:\n            for k in new_inds_mapping[j]:\n                cur_words_ids.append(k)\n        cur_words_head_ids = []\n        for j in splitted_mapping[doc_sents_lens[head_sent_id - 1] + head_id]:\n            for k in new_inds_mapping[j]:\n                cur_words_head_ids.append(k)\n        for i in cur_words_ids:\n            for j in cur_words_head_ids:\n                adj_matrix[i][j] = 1\n\n    return torch.FloatTensor(adj_matrix).to_sparse()\n\n\ndef eval(gcn_model, bert_model, step_1_model, step_2_forward, step_2_reverse, dataset, args):\n    with torch.no_grad():\n        gcn_model.eval()\n        bert_model.eval()\n        step_1_model.eval()\n        step_2_forward.eval()\n        step_2_reverse.eval()\n        '''真实结果'''\n        gold_instances = []\n        '''前向预测结果'''\n        forward_stage1_pred_aspect_result, forward_stage1_pred_aspect_with_sentiment, \\\n        forward_stage1_pred_aspect_sentiment_logit, forward_stage2_pred_opinion_result, \\\n        forward_stage2_pred_opinion_sentiment_logit = [],[],[],[],[]\n\n        '''反向预测结果'''\n        reverse_stage1_pred_opinion_result, reverse_stage1_pred_opinion_with_sentiment, \\\n        reverse_stage1_pred_opinion_sentiment_logit, reverse_stage2_pred_aspect_result, \\\n        reverse_stage2_pred_aspect_sentiment_logit = [], [], [], [], []\n        \n        tot_loss = 0\n        tot_kl_loss = 0\n\n        for j in range(dataset.batch_count):\n            tokens_tensor, attention_mask, bert_spans_tensor, spans_mask_tensor, spans_ner_label_tensor, \\\n            spans_aspect_tensor, spans_opinion_label_tensor, reverse_ner_label_tensor, reverse_opinion_tensor, \\\n            reverse_aspect_label_tensor, related_spans_tensor, sentence_length = dataset.get_batch(j)\n\n            bert_output = bert_model(input_ids=tokens_tensor, attention_mask=attention_mask)\n            \n            sentence_adj = []\n            for sent in sentence_length:\n                sentence_adj.append(make_adj_matrix(sent[0], args.max_seq_length))\n            sentence_adj = torch.cat([i.unsqueeze(0) for i in sentence_adj], axis=0).to_dense().to(args.device)\n                        \n            h_gcn, _ = gcn_model(sentence_adj, bert_output.last_hidden_state, args.device)\n            bert_out = bert_output.last_hidden_state + h_gcn # \\hat{h}\n\n            aspect_class_logits, opinion_class_logits, spans_embedding, forward_embedding, reverse_embedding, \\\n                cnn_spans_mask_tensor = step_1_model(\n                    bert_out, attention_mask, bert_spans_tensor, spans_mask_tensor,\n                    related_spans_tensor, sentence_length)\n\n            '''Batch Update'''\n            pred_aspect_logits = torch.argmax(F.softmax(aspect_class_logits, dim=2), dim=2)\n            pred_sentiment_ligits = F.softmax(aspect_class_logits, dim=2)\n            pred_aspect_logits = torch.where(spans_mask_tensor == 1, pred_aspect_logits,\n                                             torch.tensor(0).type_as(pred_aspect_logits))\n\n            reverse_pred_stage1_logits = torch.argmax(F.softmax(opinion_class_logits, dim=2), dim=2)\n            reverse_pred_sentiment_ligits = F.softmax(opinion_class_logits, dim=2)\n            reverse_pred_stage1_logits = torch.where(spans_mask_tensor == 1, reverse_pred_stage1_logits,\n                                             torch.tensor(0).type_as(reverse_pred_stage1_logits))\n\n            '''true result synthesis'''\n            gold_instances.append(dataset.get_instances(j))\n            \n            \n            all_span_opinion_tensor = []\n            step_2_opinion_class_logits = []\n            all_span_mask = []\n            all_reverse_aspect_tensor = []\n            reverse_aspect_class_logits = []\n            reverse_span_mask = []\n\n            '''Bidirectional prediction'''\n            if torch.nonzero(pred_aspect_logits, as_tuple=False).shape[0] == 0:\n#                 print(\"zero pred_aspect_logits...\")\n                forward_stage1_pred_aspect_result.append(torch.full_like(spans_aspect_tensor, -1))\n                forward_stage1_pred_aspect_with_sentiment.append(pred_aspect_logits)\n                forward_stage1_pred_aspect_sentiment_logit.append(pred_sentiment_ligits)\n                forward_stage2_pred_opinion_result.append(torch.full_like(spans_opinion_label_tensor, -1))\n                forward_stage2_pred_opinion_sentiment_logit.append(\n                    torch.full_like(spans_opinion_label_tensor.unsqueeze(-1).expand(-1, -1, len(sentiment2id)), -1))\n\n            else:\n#                 print(\"non-zero pred_aspect_logits...\")\n                pred_aspect_spans = torch.chunk(torch.nonzero(pred_aspect_logits, as_tuple=False),\n                                                torch.nonzero(pred_aspect_logits, as_tuple=False).shape[0], dim=0)\n                pred_span_aspect_tensor = None\n                for pred_aspect_span in pred_aspect_spans:\n                    batch_num = pred_aspect_span.squeeze()[0]\n                    span_aspect_tensor_unspilt_1 = bert_spans_tensor[batch_num, pred_aspect_span.squeeze()[1], :2]\n                    span_aspect_tensor_unspilt = torch.tensor(\n                        (batch_num, span_aspect_tensor_unspilt_1[0], span_aspect_tensor_unspilt_1[1])).unsqueeze(0)\n                    if pred_span_aspect_tensor is None:\n                        pred_span_aspect_tensor = span_aspect_tensor_unspilt\n                    else:\n                        pred_span_aspect_tensor = torch.cat((pred_span_aspect_tensor, span_aspect_tensor_unspilt),dim=0)\n\n                all_span_opinion_tensor, all_span_aspect_tensor, all_bert_embedding, all_attention_mask, \\\n                    all_spans_embedding, all_span_mask = stage_2_features_generation(\n                        bert_out, attention_mask, bert_spans_tensor, spans_mask_tensor,\n                        forward_embedding, pred_span_aspect_tensor)\n                \n#                 all_span_opinion_tensor, all_span_aspect_tensor, all_bert_embedding, all_attention_mask, \\\n#                 all_spans_embedding, all_span_mask = stage_2_features_generation(bert_out,\n#                                                                              attention_mask, bert_spans_tensor,\n#                                                                              spans_mask_tensor, forward_embedding,\n#                                                                              spans_aspect_tensor,\n#                                                                              spans_opinion_label_tensor)\n\n\n                step_2_opinion_class_logits, opinion_attention = step_2_forward(all_spans_embedding, all_span_mask,\n                                                                         all_span_aspect_tensor)\n\n                forward_stage1_pred_aspect_result.append(pred_span_aspect_tensor)\n                forward_stage1_pred_aspect_with_sentiment.append(pred_aspect_logits)\n                forward_stage1_pred_aspect_sentiment_logit.append(pred_sentiment_ligits)\n                forward_stage2_pred_opinion_result.append(torch.argmax(F.softmax(step_2_opinion_class_logits, dim=2), dim=2))\n                forward_stage2_pred_opinion_sentiment_logit.append(F.softmax(step_2_opinion_class_logits, dim=2))\n            '''Reverse prediction'''\n            if torch.nonzero(reverse_pred_stage1_logits, as_tuple=False).shape[0] == 0:\n#                 print(\"zero reverse_pred_stage1_logits...\")\n                reverse_stage1_pred_opinion_result.append(torch.full_like(reverse_opinion_tensor, -1))\n                reverse_stage1_pred_opinion_with_sentiment.append(reverse_pred_stage1_logits)\n                reverse_stage1_pred_opinion_sentiment_logit.append(reverse_pred_sentiment_ligits)\n                reverse_stage2_pred_aspect_result.append(torch.full_like(reverse_aspect_label_tensor, -1))\n                reverse_stage2_pred_aspect_sentiment_logit.append(\n                    torch.full_like(reverse_aspect_label_tensor.unsqueeze(-1).expand(-1, -1, len(sentiment2id)), -1))\n            else:\n#                 print(\"non-zero reverse_pred_stage1_logits...\")\n                reverse_pred_opinion_spans = torch.chunk(torch.nonzero(reverse_pred_stage1_logits, as_tuple=False),\n                                                torch.nonzero(reverse_pred_stage1_logits, as_tuple=False).shape[0], dim=0)\n                reverse_span_opinion_tensor = None\n                for reverse_pred_opinion_span in reverse_pred_opinion_spans:\n                    batch_num = reverse_pred_opinion_span.squeeze()[0]\n                    reverse_opinion_tensor_unspilt = bert_spans_tensor[batch_num, reverse_pred_opinion_span.squeeze()[1], :2]\n                    reverse_opinion_tensor_unspilt = torch.tensor(\n                        (batch_num, reverse_opinion_tensor_unspilt[0], reverse_opinion_tensor_unspilt[1])).unsqueeze(0)\n                    if reverse_span_opinion_tensor is None:\n                        reverse_span_opinion_tensor = reverse_opinion_tensor_unspilt\n                    else:\n                        reverse_span_opinion_tensor = torch.cat((reverse_span_opinion_tensor, reverse_opinion_tensor_unspilt), dim=0)\n           \n                all_reverse_aspect_tensor, all_reverse_opinion_tensor, reverse_bert_embedding, reverse_attention_mask, \\\n                reverse_spans_embedding, reverse_span_mask = stage_2_features_generation(\n                        bert_out,\n                        attention_mask,\n                        bert_spans_tensor,\n                        spans_mask_tensor,\n                        reverse_embedding,\n                        reverse_span_opinion_tensor)\n\n                reverse_aspect_class_logits, reverse_aspect_attention = step_2_reverse(reverse_spans_embedding,\n                                                                                reverse_span_mask,\n                                                                                all_reverse_opinion_tensor)\n\n                reverse_stage1_pred_opinion_result.append(reverse_span_opinion_tensor)\n                reverse_stage1_pred_opinion_with_sentiment.append(reverse_pred_stage1_logits)\n                reverse_stage1_pred_opinion_sentiment_logit.append(reverse_pred_sentiment_ligits)\n                reverse_stage2_pred_aspect_result.append(torch.argmax(F.softmax(reverse_aspect_class_logits, dim=2), dim=2))\n                reverse_stage2_pred_aspect_sentiment_logit.append(F.softmax(reverse_aspect_class_logits, dim=2))\n            \n#             print('val all_span_opinion_tensor', all_span_opinion_tensor)\n#                 step_2_opinion_class_logits, \\\n#                 all_span_mask, \\\n#                 all_reverse_aspect_tensor, \\\n#                 reverse_aspect_class_logits, \\\n#                 reverse_span_mask)\n            if not(all_span_opinion_tensor is None or not len(all_span_opinion_tensor) or \\\n                step_2_opinion_class_logits is None or not len(step_2_opinion_class_logits) or \\\n                all_span_mask is None or not len(all_span_mask) or \\\n                all_reverse_aspect_tensor is None or not len(all_reverse_aspect_tensor) or \\\n                reverse_aspect_class_logits is None or not len(reverse_aspect_class_logits) or \\\n                reverse_span_mask is None or not len(reverse_span_mask)):\n                print(\"val evaluating loss...\")\n                loss, kl_loss = Loss(spans_ner_label_tensor, aspect_class_logits, all_span_opinion_tensor, step_2_opinion_class_logits,\n                            spans_mask_tensor, all_span_mask, reverse_ner_label_tensor, opinion_class_logits,\n                            all_reverse_aspect_tensor, reverse_aspect_class_logits, cnn_spans_mask_tensor, reverse_span_mask,\n                            spans_embedding, related_spans_tensor, args)\n                print(\"val tot_loss...{}\".format(loss.item()))\n                tot_loss += loss.item()\n                tot_kl_loss += kl_loss\n\n        gold_instances = [x for i in gold_instances for x in i]\n        forward_pred_data = (forward_stage1_pred_aspect_result, forward_stage1_pred_aspect_with_sentiment,\n                             forward_stage1_pred_aspect_sentiment_logit, forward_stage2_pred_opinion_result,\n                             forward_stage2_pred_opinion_sentiment_logit)\n        forward_pred_result = unbatch_data(forward_pred_data)\n\n        reverse_pred_data = (reverse_stage1_pred_opinion_result, reverse_stage1_pred_opinion_with_sentiment,\n                             reverse_stage1_pred_opinion_sentiment_logit, reverse_stage2_pred_aspect_result,\n                             reverse_stage2_pred_aspect_sentiment_logit)\n        reverse_pred_result = unbatch_data(reverse_pred_data)\n\n        metric = Metric(args, forward_pred_result, reverse_pred_result, gold_instances)\n        aspect_result, opinion_result, apce_result, pair_result, triplet_result = metric.score_triples()\n\n        \n        logger.info(\n            'aspect precision: {}\\taspect recall: {:.8f}\\taspect f1: {:.8f}'.format(aspect_result[0], aspect_result[1], aspect_result[2]))\n        logger.info(\n            'opinion precision: {}\\topinion recall: {:.8f}\\topinion f1: {:.8f}'.format(opinion_result[0],\n                                                                                        opinion_result[1],\n                                                                                        opinion_result[2]))\n        logger.info('APCE precision: {}\\tAPCE recall: {:.8f}\\tAPCE f1: {:.8f}'.format(apce_result[0],\n                                                                                apce_result[1], apce_result[2]))\n        logger.info('pair precision: {}\\tpair recall: {:.8f}\\tpair f1: {:.8f}'.format(pair_result[0],\n                                                                                          pair_result[1],\n                                                                                          pair_result[2]))\n        logger.info('triple precision: {}\\ttriple recall: {:.8f}\\ttriple f1: {:.8f}'.format(triplet_result[0],\n                                                                                          triplet_result[1],\n                                                                                          triplet_result[2]))\n\n    return aspect_result, opinion_result, apce_result, pair_result, triplet_result, tot_loss\n\n\ndef train(args):\n#     wandb_ran = wandb.init(\n#         project='aste-SBN',\n#         config=args\n#     )\n\n    if args.dataset_path == './datasets/BIO_form/':\n        train_path = args.dataset_path + args.dataset + \"/train.json\"\n        dev_path = args.dataset_path + args.dataset + \"/dev.json\"\n        test_path = args.dataset_path + args.dataset + \"/test.json\"\n    else:\n        train_path = args.dataset_path + args.dataset + \"/train_full.txt\"\n        dev_path = args.dataset_path + args.dataset + \"/dev_full.txt\"\n#         test_path = args.dataset_path + args.dataset + \"/test_full_fake_only.txt\"\n        test_path = '/kaggle/input/dataset-sent-no-fake/test_full_fake_only.txt'\n\n    print('-------------------------------')\n    print('Start loading the test set')\n    logger.info('Start loading the test set')\n    test_datasets = MyDataset(args, test_path, if_train=False)\n    testset = DataTterator(test_datasets, args)\n    print('The test set is loaded')\n    logger.info('The test set is loaded')\n    print('-------------------------------')\n    \n    gcn = GCN(emb_dim=args.bert_feature_dim).to(args.device)\n    gcn_param_optimizer = list(gcn.named_parameters())\n#     gcn = None\n\n    Bert = BertModel.from_pretrained(args.init_model)\n    bert_config = Bert.config\n\n    if args.add_pos_enc:\n        print(\"Change pos_embeddings to 1536 len...\")\n\n        # word_emb\n        word_emb = Bert.embeddings.word_embeddings.weight.data\n\n        # token_type_emb\n        token_type_emb = Bert.embeddings.token_type_embeddings.weight.data\n\n        # pos_enc\n        pos_enc = Bert.embeddings.position_embeddings.weight.data\n        new_pos_enc = torch.concat((pos_enc, pos_enc * 2, pos_enc * 4), axis=0)\n#         new_pos_enc = torch.repeat_interleave(pos_enc, 3, dim=0)\n\n        # new config and embeddings structure\n        bert_config.update({'max_position_embeddings': 1536})\n        Bert.embeddings = BertEmbeddings(bert_config)\n\n        # return pretrained weights\n        Bert.embeddings.word_embeddings.weight.data = word_emb\n        Bert.embeddings.token_type_embeddings.weight.data = token_type_emb\n        Bert.embeddings.position_embeddings.weight.data = new_pos_enc\n\n        print(\"Changed successful!\")\n    \n    Bert.to(args.device)\n    bert_param_optimizer = list(Bert.named_parameters())\n\n    step_1_model = Step_1(args, bert_config)\n    step_1_model.to(args.device)\n    step_1_param_optimizer = list(step_1_model.named_parameters())\n\n    step2_forward_model = Step_2_forward(args, bert_config)\n    step2_forward_model.to(args.device)\n    forward_step2_param_optimizer = list(step2_forward_model.named_parameters())\n\n    step2_reverse_model = Step_2_reverse(args, bert_config)\n    step2_reverse_model.to(args.device)\n    reverse_step2_param_optimizer = list(step2_reverse_model.named_parameters())\n\n    training_param_optimizer = [\n        {'params': [p for n, p in gcn_param_optimizer]},\n        {'params': [p for n, p in bert_param_optimizer]},\n        {'params': [p for n, p in step_1_param_optimizer], 'lr': args.task_learning_rate},\n        {'params': [p for n, p in forward_step2_param_optimizer], 'lr': args.task_learning_rate},\n        {'params': [p for n, p in reverse_step2_param_optimizer], 'lr': args.task_learning_rate}]\n    optimizer = AdamW(training_param_optimizer, lr=args.learning_rate)\n\n    if args.model_to_upload != None:\n        \n        model_path = args.model_to_upload\n        if args.device == 'cpu':\n            state = torch.load(model_path, map_location=torch.device('cpu'))\n        else:\n            state = torch.load(model_path)\n        \n        new_state = {}\n        for i in state['bert_model'].keys():\n            new_state[i[7:]] = state['bert_model'][i]\n        Bert.load_state_dict(new_state)\n        \n        new_state = {}\n        for i in state['step_1_model'].keys():\n            new_state[i[7:]] = state['step_1_model'][i]\n        step_1_model.load_state_dict(new_state)\n        \n        new_state = {}\n        for i in state['step2_forward_model'].keys():\n            new_state[i[7:]] = state['step2_forward_model'][i]\n        step2_forward_model.load_state_dict(new_state)\n        \n        new_state = {}\n        for i in state['step2_reverse_model'].keys():\n            new_state[i[7:]] = state['step2_reverse_model'][i]\n        step2_reverse_model.load_state_dict(new_state)\n        \n        optimizer.load_state_dict(state['optimizer'])\n        \n        with torch.no_grad():\n            Bert.eval()\n            step_1_model.eval()\n            step2_forward_model.eval()\n            step2_reverse_model.eval()\n\n\n    if args.multi_gpu:\n        gcn = torch.nn.DataParallel(gcn)\n        Bert = torch.nn.DataParallel(Bert)\n        step_1_model = torch.nn.DataParallel(step_1_model)\n        step2_forward_model = torch.nn.DataParallel(step2_forward_model)\n        step2_reverse_model = torch.nn.DataParallel(step2_reverse_model)\n\n    if args.mode == 'train':\n        print('-------------------------------')\n        logger.info('Start loading the training and verification set')\n        print('Start loading the training and verification set')\n        train_datasets = MyDataset(args, train_path, if_train=True)\n        trainset = DataTterator(train_datasets, args)\n        print(\"Train features build completed\")\n\n        print(\"Dev features build beginning\")\n        dev_datasets = MyDataset(args, dev_path, if_train=False)\n        devset = DataTterator(dev_datasets, args)\n        print('The training set and verification set are loaded')\n        logger.info('The training set and verification set are loaded')\n        print('-------------------------------')\n        if not os.path.exists(args.model_dir):\n            os.makedirs(args.model_dir)\n\n        # scheduler\n        if args.whether_warm_up:\n            training_steps = args.epochs * trainset.batch_count\n            warmup_steps = int(training_steps * args.warm_up)\n            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n                                                        num_training_steps=training_steps)\n\n        tot_loss = 0\n        tot_kl_loss = 0\n        best_aspect_f1, best_opinion_f1, best_APCE_f1, best_pairs_f1, best_triple_f1 = 0,0,0,0,0\n        best_pairs_f1 = 0\n        for i in range(args.epochs):\n            logger.info(('Epoch:{}'.format(i)))\n            for j in tqdm.trange(trainset.batch_count):\n                \n                gcn.train()\n                Bert.train()\n                step_1_model.train()\n                step2_forward_model.train()\n                step2_reverse_model.train()\n                \n                if j == 1:\n                    start = time.time()\n                optimizer.zero_grad()\n\n                tokens_tensor, attention_mask, bert_spans_tensor, spans_mask_tensor, spans_ner_label_tensor, \\\n                spans_aspect_tensor, spans_opinion_label_tensor, reverse_ner_label_tensor, reverse_opinion_tensor, \\\n                reverse_aspect_label_tensor, related_spans_tensor, sentence_length = trainset.get_batch(j)\n                \n                sentence_adj = []\n                for sent in sentence_length:\n                    sentence_adj.append(make_adj_matrix(sent[0], args.max_seq_length))\n                sentence_adj = torch.cat([i.unsqueeze(0) for i in sentence_adj], axis=0).to_dense().to(args.device)\n                    \n                bert_output = Bert(input_ids=tokens_tensor, attention_mask=attention_mask)\n                \n                h_gcn, _ = gcn(sentence_adj, bert_output.last_hidden_state, args.device)\n                bert_out = bert_output.last_hidden_state + h_gcn # \\hat{h}\n#                 bert_out = torch.cat((bert_output.last_hidden_state, h_gcn), dim=2) # \\hat{h}\n                \n                aspect_class_logits, opinion_class_logits, spans_embedding, forward_embedding, reverse_embedding, \\\n                    cnn_spans_mask_tensor = step_1_model(bert_out,\n                                                        attention_mask,\n                                                        bert_spans_tensor,\n                                                        spans_mask_tensor,\n                                                        related_spans_tensor,\n                                                        sentence_length)\n\n                '''Batch Update'''\n                all_span_opinion_tensor, all_span_aspect_tensor, all_bert_embedding, all_attention_mask, \\\n                all_spans_embedding, all_span_mask = stage_2_features_generation(bert_out,\n                                                                             attention_mask, bert_spans_tensor,\n                                                                             spans_mask_tensor, forward_embedding,\n                                                                             spans_aspect_tensor,\n                                                                             spans_opinion_label_tensor)\n                all_reverse_aspect_tensor, all_reverse_opinion_tensor, reverse_bert_embedding, reverse_attention_mask, \\\n                reverse_spans_embedding, reverse_span_mask = stage_2_features_generation(bert_out,\n                                                                                     attention_mask, bert_spans_tensor,\n                                                                                     spans_mask_tensor, reverse_embedding,\n                                                                                     reverse_opinion_tensor,\n                                                                                     reverse_aspect_label_tensor)\n\n                step_2_opinion_class_logits, opinion_attention = step2_forward_model(all_spans_embedding, \n                                                                                     all_span_mask, all_span_aspect_tensor)\n                step_2_aspect_class_logits, aspect_attention = step2_reverse_model(reverse_spans_embedding,\n                    reverse_span_mask, all_reverse_opinion_tensor)\n                \n#                 print('train all_span_opinion_tensor', all_span_opinion_tensor)\n\n                loss, kl_loss = Loss(spans_ner_label_tensor, aspect_class_logits, all_span_opinion_tensor, step_2_opinion_class_logits,\n                            spans_mask_tensor, all_span_mask, reverse_ner_label_tensor, opinion_class_logits,\n                            all_reverse_aspect_tensor, step_2_aspect_class_logits, cnn_spans_mask_tensor, reverse_span_mask,\n                            spans_embedding, related_spans_tensor, args)\n                \n                if args.accumulation_steps > 1:\n                    loss = loss / args.accumulation_steps\n                    loss.backward()\n                    if ((j + 1) % args.accumulation_steps) == 0:\n                        optimizer.step()\n                        if args.whether_warm_up:\n                            scheduler.step()\n                else:\n                    loss.backward()\n                    optimizer.step()\n                    if args.whether_warm_up:\n                        scheduler.step()\n                tot_loss += loss.item()\n                tot_kl_loss += kl_loss\n            \n            \n            logger.info(('Loss:', tot_loss))\n            logger.info(('KL_Loss:', tot_kl_loss))\n            \n\n            print('Evaluating, please wait')\n            aspect_result, opinion_result, apce_result, pair_result, triplet_result, val_tot_loss = eval(gcn, Bert, step_1_model,\n                                                                                           step2_forward_model,\n                                                                                           step2_reverse_model,\n                                                                                           devset, args)\n\n            wandb.log({\n                'Loss':tot_loss,\n                'KL_Loss':tot_kl_loss,\n                'Val_Loss':val_tot_loss,\n                'triple precision':triplet_result[0],\n                'triple recall':triplet_result[1],\n                'triple f1':triplet_result[2]\n            })\n\n            tot_loss = 0\n            tot_kl_loss = 0\n\n            print('Evaluating complete')\n\n\n            if triplet_result[2] > 0.5:\n#                 model_path = \"/kaggle/working/SBN_models/1904_base_full_model_gcn_\" + str(i) +'_'+ str(triplet_result[2]) + '.pt'\n#                 state = {\n# #                     \"gcn_model\": gcn.state_dict(),\n#                     \"bert_model\": Bert.state_dict(),\n#                     \"step_1_model\": step_1_model.state_dict(),\n#                     \"step2_forward_model\": step2_forward_model.state_dict(),\n#                     \"step2_reverse_model\": step2_reverse_model.state_dict(),\n#                     \"optimizer\": optimizer.state_dict()\n#                 }\n#                 torch.save(state, model_path)\n#                 logger.info(\"_________________________________________________________\")\n#                 logger.info(\"best model save\")\n#                 logger.info(\"_________________________________________________________\")\n\n                best_triple_f1 = triplet_result[2]\n                best_triple_precision = triplet_result[0]\n                best_triple_recall = triplet_result[1]\n                best_triple_epoch = i\n                \n                print(\"Test results...\")\n                eval(gcn, Bert, step_1_model, step2_forward_model, step2_reverse_model, testset, args)\n\n    logger.info(\"Features build completed\")\n    logger.info(\"Evaluation on testset:\")\n\n    eval(gcn, Bert, step_1_model, step2_forward_model, step2_reverse_model, testset, args)\n    wandb.finish()\n\n\ndef test(args):\n#     test_path = '/kaggle/input/dataset-sentences-with-fake-start/test_no_right_answers.txt'\n    test_path = args.dataset_path\n\n    print('-------------------------------')\n    print('Start loading the test set')\n    logger.info('Start loading the test set')\n    test_datasets = MyDataset(args, test_path, if_train=False)\n    testset = DataTterator(test_datasets, args)\n    print('The test set is loaded')\n    logger.info('The test set is loaded')\n    print('-------------------------------')\n\n    print('Start loading model...')\n\n    model_path = args.model_to_upload\n    if args.device == 'cpu':\n        state = torch.load(model_path, map_location=torch.device('cpu'))\n    else:\n        state = torch.load(model_path)\n        # state = load_with_single_gpu(model_path)\n\n    Bert = BertModel.from_pretrained(args.init_model)\n    bert_config = Bert.config\n    \n    if args.add_pos_enc:\n        print(\"Change pos_embeddings to 1536 len...\")\n\n        # word_emb\n        word_emb = Bert.embeddings.word_embeddings.weight.data\n\n        # token_type_emb\n        token_type_emb = Bert.embeddings.token_type_embeddings.weight.data\n\n        # pos_enc\n        pos_enc = Bert.embeddings.position_embeddings.weight.data\n        new_pos_enc = torch.concat((pos_enc, pos_enc, pos_enc), axis=0)\n\n        # new config and embeddings structure\n        bert_config.update({'max_position_embeddings': 1536})\n        Bert.embeddings = BertEmbeddings(bert_config)\n\n        # return pretrained weights\n        Bert.embeddings.word_embeddings.weight.data = word_emb\n        Bert.embeddings.token_type_embeddings.weight.data = token_type_emb\n        Bert.embeddings.position_embeddings.weight.data = new_pos_enc\n\n        print(\"Changed successful!\")\n    \n    \n    Bert.to(args.device)\n    bert_param_optimizer = list(Bert.named_parameters())\n\n    step_1_model = Step_1(args, bert_config)\n    step_1_model.to(args.device)\n    step_1_param_optimizer = list(step_1_model.named_parameters())\n\n    step2_forward_model = Step_2_forward(args, bert_config)\n    step2_forward_model.to(args.device)\n    forward_step2_param_optimizer = list(step2_forward_model.named_parameters())\n\n    step2_reverse_model = Step_2_reverse(args, bert_config)\n    step2_reverse_model.to(args.device)\n    reverse_step2_param_optimizer = list(step2_reverse_model.named_parameters())\n\n    training_param_optimizer = [\n        {'params': [p for n, p in bert_param_optimizer]},\n        {'params': [p for n, p in step_1_param_optimizer], 'lr': args.task_learning_rate},\n        {'params': [p for n, p in forward_step2_param_optimizer], 'lr': args.task_learning_rate},\n        {'params': [p for n, p in reverse_step2_param_optimizer], 'lr': args.task_learning_rate}]\n    optimizer = AdamW(training_param_optimizer, lr=args.learning_rate)\n\n    \n    if args.model_to_upload != None:\n        model_path = args.model_to_upload\n        if args.device == 'cpu':\n            state = torch.load(model_path, map_location=torch.device('cpu'))\n        else:\n            state = torch.load(model_path)\n        \n        new_state = {}\n        for i in state['bert_model'].keys():\n            new_state[i[7:]] = state['bert_model'][i]\n        Bert.load_state_dict(new_state)\n        \n        new_state = {}\n        for i in state['step_1_model'].keys():\n            new_state[i[7:]] = state['step_1_model'][i]\n        step_1_model.load_state_dict(new_state)\n        \n        new_state = {}\n        for i in state['step2_forward_model'].keys():\n            new_state[i[7:]] = state['step2_forward_model'][i]\n        step2_forward_model.load_state_dict(new_state)\n        \n        new_state = {}\n        for i in state['step2_reverse_model'].keys():\n            new_state[i[7:]] = state['step2_reverse_model'][i]\n        step2_reverse_model.load_state_dict(new_state)\n        \n        optimizer.load_state_dict(state['optimizer'])\n        with torch.no_grad():\n            Bert.eval()\n            step_1_model.eval()\n            step2_forward_model.eval()\n            step2_reverse_model.eval()\n\n\n    if args.multi_gpu:\n        Bert = torch.nn.DataParallel(Bert)\n        step_1_model = torch.nn.DataParallel(step_1_model)\n        step2_forward_model = torch.nn.DataParallel(step2_forward_model)\n        step2_reverse_model = torch.nn.DataParallel(step2_reverse_model)\n\n    print(\"Model loading ended\")\n\n    eval(Bert, step_1_model, step2_forward_model, step2_reverse_model, testset, args)\n\n\n\ndef load_with_single_gpu(model_path):\n    state_dict = torch.load(model_path)\n    from collections import OrderedDict\n    new_state_dict = OrderedDict()\n    final_state = {}\n    for i in state_dict:\n        for k, v in state_dict[i].items():\n            name = k[7:]\n            new_state_dict[name] = v\n        final_state[i] = new_state_dict\n        new_state_dict = OrderedDict()\n    return  final_state\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Train scrip\")\n    parser.add_argument('--model_dir', type=str, default=\"savemodel/\", help='model path prefix')\n    parser.add_argument('--model_to_upload', type=str, default=None)\n    parser.add_argument('--add_pos_enc', default=False)\n    parser.add_argument('--device', type=str, default=\"cuda\", help='cuda or cpu')\n    parser.add_argument(\"--init_model\", default=\"pretrained_models/bert-base-uncased\", type=str, required=False,help=\"Initial model.\")\n    parser.add_argument(\"--init_vocab\", default=\"pretrained_models/bert-base-uncased\", type=str, required=False,help=\"Initial vocab.\")\n\n    parser.add_argument(\"--bert_feature_dim\", default=768, type=int, help=\"feature dim for bert\")\n    parser.add_argument(\"--do_lower_case\", default=True, action='store_true',help=\"Set this flag if you are using an uncased model.\")\n    parser.add_argument(\"--max_seq_length\", default=100, type=int,help=\"The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded.\")\n    parser.add_argument(\"--drop_out\", type=int, default=0.1, help=\"\")\n    parser.add_argument(\"--max_span_length\", type=int, default=8, help=\"\")\n    parser.add_argument(\"--embedding_dim4width\", type=int, default=200,help=\"\")\n    parser.add_argument(\"--task_learning_rate\", type=float, default=1e-4)\n    parser.add_argument(\"--learning_rate\", type=float, default=1e-5)\n    parser.add_argument(\"--accumulation_steps\", type=int, default=1)\n    parser.add_argument(\"--multi_gpu\", default=False)\n    parser.add_argument('--epochs', type=int, default=130, help='training epoch number')\n    parser.add_argument(\"--train_batch_size\", default=16, type=int, help=\"batch size for training\")\n    parser.add_argument(\"--RANDOM_SEED\", type=int, default=2022, help=\"\")\n    '''修改了数据格式'''\n    parser.add_argument(\"--dataset_path\", default=\"\",\n                        help=\"\")\n    parser.add_argument(\"--dataset\", default=\"\", type=str,\n                        help=\"specify the dataset\")\n    parser.add_argument('--mode', type=str, default=\"test\", choices=[\"train\", \"test\"], help='option: train, test')\n    '''对相似Span进行attention'''\n    # 分词中仅使用结果的首token\n    parser.add_argument(\"--Only_token_head\", default=False)\n    # Choose the synthesis method of Span\n    parser.add_argument('--span_generation', type=str, default=\"Max\", choices=[\"Start_end\", \"Max\", \"Average\", \"CNN\", \"ATT\"],\n                        help='option: CNN, Max, Start_end, Average, ATT, SE_ATT')\n    parser.add_argument('--ATT_SPAN_block_num', type=int, default=1, help=\"number of block in generating spans\")\n\n    # Whether to add a separation loss to the relevant span\n    parser.add_argument(\"--kl_loss\", default=True)\n    parser.add_argument(\"--kl_loss_weight\", type=int, default=0.5, help=\"weight of the kl_loss\")\n    parser.add_argument('--kl_loss_mode', type=str, default=\"KLLoss\", choices=[\"KLLoss\", \"JSLoss\", \"EMLoss, CSLoss\"],\n                        help='选择分离相似Span的分离函数, KL散度、JS散度、欧氏距离以及余弦相似度')\n    # Whether to use the filtering algorithm in the test\n    parser.add_argument('--Filter_Strategy',  default=True, help='是否使用筛选算法去除冲突三元组')\n    # Deprecated    Related Span attention\n    parser.add_argument(\"--related_span_underline\", default=False)\n    parser.add_argument(\"--related_span_block_num\", type=int, default=1, help=\"number of block in related span attention\")\n\n    # choose Cross Select the number of ATT blocks in Attention\n    parser.add_argument(\"--block_num\", type=int, default=1, help=\"number of block\")\n    parser.add_argument(\"--output_path\", default='triples.json')\n    # Enter and sort in the order of sentences\n    parser.add_argument(\"--order_input\", default=True, help=\"\")\n    '''Randomize input span sorting'''\n    parser.add_argument(\"--random_shuffle\", type=int, default=0, help=\"\")\n    # Verify model complexity\n    parser.add_argument(\"--model_para_test\", default=False)\n    # Use Warm up to converge quickly\n    parser.add_argument('--whether_warm_up', default=False)\n    parser.add_argument('--warm_up', type=float, default=0.1)\n    args = parser.parse_args()\n\n    for k,v in sorted(vars(args).items()):\n        logger.info(str(k) + '=' + str(v))\n    if args.mode == 'train':\n        train(args)\n    else:\n        test(args)\n\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except KeyboardInterrupt:\n        logger.info(\"keyboard break\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:42:38.684345Z","iopub.execute_input":"2024-04-30T10:42:38.684748Z","iopub.status.idle":"2024-04-30T10:42:38.719721Z","shell.execute_reply.started":"2024-04-30T10:42:38.684717Z","shell.execute_reply":"2024-04-30T10:42:38.718808Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Overwriting run.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir SBN_models","metadata":{"execution":{"iopub.status.busy":"2024-04-30T09:24:45.346648Z","iopub.execute_input":"2024-04-30T09:24:45.346888Z","iopub.status.idle":"2024-04-30T09:24:46.299564Z","shell.execute_reply.started":"2024-04-30T09:24:45.346867Z","shell.execute_reply":"2024-04-30T09:24:46.298310Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"!mkdir log","metadata":{"execution":{"iopub.status.busy":"2024-04-30T09:24:46.301090Z","iopub.execute_input":"2024-04-30T09:24:46.301699Z","iopub.status.idle":"2024-04-30T09:24:47.249922Z","shell.execute_reply.started":"2024-04-30T09:24:46.301668Z","shell.execute_reply":"2024-04-30T09:24:47.248782Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"%%writefile log.py\n\n\nimport logging\nfrom datetime import datetime\n\nnow = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\nlogger = logging.getLogger(\"test\")\nlogger.setLevel(level=logging.INFO)\n\nhandler = logging.FileHandler(\"log/\"+now+\".log\", encoding='utf-8')\nhandler.setLevel(logging.INFO)\n\nconsole = logging.StreamHandler()\nconsole.setLevel(logging.INFO)\n\nlogger.addHandler(handler)\nlogger.addHandler(console)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T09:24:47.251319Z","iopub.execute_input":"2024-04-30T09:24:47.251609Z","iopub.status.idle":"2024-04-30T09:24:47.257837Z","shell.execute_reply.started":"2024-04-30T09:24:47.251581Z","shell.execute_reply":"2024-04-30T09:24:47.256804Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Writing log.py\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb","metadata":{"execution":{"iopub.status.busy":"2024-04-30T09:24:47.259213Z","iopub.execute_input":"2024-04-30T09:24:47.259520Z","iopub.status.idle":"2024-04-30T09:24:48.481129Z","shell.execute_reply.started":"2024-04-30T09:24:47.259498Z","shell.execute_reply":"2024-04-30T09:24:48.480396Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"wandb.login(key='')","metadata":{"execution":{"iopub.status.busy":"2024-04-30T09:24:48.482541Z","iopub.execute_input":"2024-04-30T09:24:48.482928Z","iopub.status.idle":"2024-04-30T09:24:49.632691Z","shell.execute_reply.started":"2024-04-30T09:24:48.482903Z","shell.execute_reply":"2024-04-30T09:24:49.631689Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!python run.py --dataset_path '/kaggle/input/dataset-sent-no-fake' --init_model 'ai-forever/ruBert-base' \\\n  --init_vocab 'ai-forever/ruBert-base' --mode 'train' \\\n  --multi_gpu True \\\n  --kl_loss False \\\n  --max_span_length 1 --train_batch_size 24 --max_seq_length 512 \\\n  --bert_feature_dim 768 --epochs 20","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:43:08.800457Z","iopub.execute_input":"2024-04-30T10:43:08.801102Z","iopub.status.idle":"2024-04-30T10:50:23.516792Z","shell.execute_reply.started":"2024-04-30T10:43:08.801072Z","shell.execute_reply":"2024-04-30T10:50:23.515589Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"2024-04-30 10:43:11.014054: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-30 10:43:11.014114: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-30 10:43:11.015866: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nATT_SPAN_block_num=1\nFilter_Strategy=True\nOnly_token_head=False\nRANDOM_SEED=2022\naccumulation_steps=1\nadd_pos_enc=False\nbert_feature_dim=768\nblock_num=1\ndataset=\ndataset_path=/kaggle/input/dataset-sent-no-fake\ndevice=cuda\ndo_lower_case=True\ndrop_out=0.1\nembedding_dim4width=200\nepochs=20\ninit_model=ai-forever/ruBert-base\ninit_vocab=ai-forever/ruBert-base\nkl_loss=False\nkl_loss_mode=KLLoss\nkl_loss_weight=0.5\nlearning_rate=1e-05\nmax_seq_length=512\nmax_span_length=1\nmode=train\nmodel_dir=savemodel/\nmodel_para_test=False\nmodel_to_upload=None\nmulti_gpu=True\norder_input=True\noutput_path=triples.json\nrandom_shuffle=0\nrelated_span_block_num=1\nrelated_span_underline=False\nspan_generation=Max\ntask_learning_rate=0.0001\ntrain_batch_size=24\nwarm_up=0.1\nwhether_warm_up=False\n-------------------------------\nStart loading the test set\nStart loading the test set\nThe test set is loaded\nThe test set is loaded\n-------------------------------\nSome weights of the model checkpoint at ai-forever/ruBert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n-------------------------------\nStart loading the training and verification set\nStart loading the training and verification set\nSingle sentence 47 The middle triplet reappears!\nSingle sentence 507 The middle triplet reappears!\nSingle sentence 551 The middle triplet reappears!\nSingle sentence 654 The middle triplet reappears!\nSingle sentence 696 The middle triplet reappears!\nSingle sentence 877 The middle triplet reappears!\nSingle sentence 1014 The middle triplet reappears!\nSingle sentence 1205 The middle triplet reappears!\nSingle sentence 1504 The middle triplet reappears!\nSingle sentence 1866 The middle triplet reappears!\nSingle sentence 1881 The middle triplet reappears!\nSingle sentence 1903 The middle triplet reappears!\nSingle sentence 1923 The middle triplet reappears!\nSingle sentence 1989 The middle triplet reappears!\nSingle sentence 2179 The middle triplet reappears!\nSingle sentence 2625 The middle triplet reappears!\nSingle sentence 2786 The middle triplet reappears!\nSingle sentence 3489 The middle triplet reappears!\nTrain features build completed\nDev features build beginning\nSingle sentence 340 The middle triplet reappears!\nThe training set and verification set are loaded\nThe training set and verification set are loaded\n-------------------------------\nEpoch:0\n100%|█████████████████████████████████████████| 147/147 [05:21<00:00,  2.19s/it]\n('Loss:', 70652.5779953003)\n('KL_Loss:', 844.7090110967998)\nEvaluating, please wait\nReady to for\n100%|████████████████████████████████████████| 771/771 [00:03<00:00, 205.07it/s]\naspect precision: 0.7515212981744422\taspect recall: 0.64772727\taspect f1: 0.69577465\nopinion precision: 0.7131147540983607\topinion recall: 0.52847380\topinion f1: 0.60706498\nAPCE precision: 0.7790868924889544\tAPCE recall: 0.46241259\tAPCE f1: 0.58036204\npair precision: 0.40412186379928317\tpair recall: 0.33235077\tpair f1: 0.36473918\ntriple precision: 0.4014336917562724\ttriple recall: 0.33014001\ttriple f1: 0.36231298\nTraceback (most recent call last):\n  File \"/kaggle/working/run.py\", line 845, in <module>\n    main()\n  File \"/kaggle/working/run.py\", line 838, in main\n    train(args)\n  File \"/kaggle/working/run.py\", line 596, in train\n    wandb.log({\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/preinit.py\", line 36, in preinit_wrapper\n    raise wandb.Error(f\"You must call wandb.init() before {name}()\")\nwandb.errors.Error: You must call wandb.init() before wandb.log()\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python run.py --dataset_path '/kaggle/input/datset-full-no-fake' --init_model 'ai-forever/ruBert-base' \\\n#   --init_vocab 'ai-forever/ruBert-base' --mode 'train' \\\n#   --add_pos_enc True \\\n#   --multi_gpu True \\\n#   --max_span_length 2 --train_batch_size 4 --max_seq_length 1408 \\\n#   --bert_feature_dim 768 --epochs 20","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:59:08.326353Z","iopub.execute_input":"2024-04-29T12:59:08.326973Z","iopub.status.idle":"2024-04-29T18:15:05.778956Z","shell.execute_reply.started":"2024-04-29T12:59:08.326936Z","shell.execute_reply":"2024-04-29T18:15:05.777858Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"2024-04-29 12:59:10.526698: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-29 12:59:10.526753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-29 12:59:10.528416: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nATT_SPAN_block_num=1\nFilter_Strategy=True\nOnly_token_head=False\nRANDOM_SEED=2022\naccumulation_steps=1\nadd_pos_enc=True\nbert_feature_dim=768\nblock_num=1\ndataset=\ndataset_path=/kaggle/input/datset-full-no-fake\ndevice=cuda\ndo_lower_case=True\ndrop_out=0.1\nembedding_dim4width=200\nepochs=20\ninit_model=ai-forever/ruBert-base\ninit_vocab=ai-forever/ruBert-base\nkl_loss=True\nkl_loss_mode=KLLoss\nkl_loss_weight=0.5\nlearning_rate=1e-05\nmax_seq_length=1408\nmax_span_length=2\nmode=train\nmodel_dir=savemodel/\nmodel_para_test=False\nmodel_to_upload=None\nmulti_gpu=True\norder_input=True\noutput_path=triples.json\nrandom_shuffle=0\nrelated_span_block_num=1\nrelated_span_underline=False\nspan_generation=Max\ntask_learning_rate=0.0001\ntrain_batch_size=4\nwarm_up=0.1\nwhether_warm_up=False\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlmartinson\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.21\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240429_125936-u98be4tg\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgolden-frost-125\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lmartinson/aste-SBN\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lmartinson/aste-SBN/runs/u98be4tg\u001b[0m\n-------------------------------\nStart loading the test set\nStart loading the test set\nThe test set is loaded\n-------------------------------\nThe test set is loaded\nSome weights of the model checkpoint at ai-forever/ruBert-base were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nChange pos_embeddings to 1536 len...\nChanged successful!\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nStart loading the training and verification set\n-------------------------------\nStart loading the training and verification set\nSingle sentence 3 The middle triplet reappears!\nSingle sentence 3 The middle triplet reappears!\nSingle sentence 86 The middle triplet reappears!\nSingle sentence 153 The middle triplet reappears!\nSingle sentence 162 The middle triplet reappears!\nSingle sentence 165 The middle triplet reappears!\nSingle sentence 177 The middle triplet reappears!\nSingle sentence 251 The middle triplet reappears!\nSingle sentence 256 The middle triplet reappears!\nSingle sentence 355 The middle triplet reappears!\nSingle sentence 529 The middle triplet reappears!\nSingle sentence 577 The middle triplet reappears!\nSingle sentence 657 The middle triplet reappears!\nSingle sentence 813 The middle triplet reappears!\nSingle sentence 833 The middle triplet reappears!\nSingle sentence 1039 The middle triplet reappears!\nSingle sentence 1044 The middle triplet reappears!\nSingle sentence 1058 The middle triplet reappears!\nSingle sentence 1089 The middle triplet reappears!\nSingle sentence 1090 The middle triplet reappears!\nSingle sentence 1090 The middle triplet reappears!\nSingle sentence 1283 The middle triplet reappears!\nSingle sentence 1291 The middle triplet reappears!\nSingle sentence 1316 The middle triplet reappears!\nSingle sentence 1405 The middle triplet reappears!\nSingle sentence 1405 The middle triplet reappears!\nSingle sentence 1425 The middle triplet reappears!\nSingle sentence 1469 The middle triplet reappears!\nSingle sentence 1554 The middle triplet reappears!\nSingle sentence 1583 The middle triplet reappears!\nSingle sentence 1609 The middle triplet reappears!\nSingle sentence 1656 The middle triplet reappears!\nSingle sentence 1665 The middle triplet reappears!\nSingle sentence 1699 The middle triplet reappears!\nSingle sentence 1711 The middle triplet reappears!\nSingle sentence 1834 The middle triplet reappears!\nSingle sentence 2066 The middle triplet reappears!\nSingle sentence 2123 The middle triplet reappears!\nSingle sentence 2239 The middle triplet reappears!\nSingle sentence 2276 The middle triplet reappears!\nSingle sentence 2276 The middle triplet reappears!\nTrain features build completed\nDev features build beginning\nSingle sentence 5 The middle triplet reappears!\nSingle sentence 19 The middle triplet reappears!\nSingle sentence 48 The middle triplet reappears!\nSingle sentence 217 The middle triplet reappears!\nSingle sentence 224 The middle triplet reappears!\nThe training set and verification set are loaded\nThe training set and verification set are loaded\n-------------------------------\nEpoch:0\n100%|█████████████████████████████████████████| 571/571 [13:36<00:00,  1.43s/it]\n('Loss:', 139286.98245620728)\n('KL_Loss:', 3379.16945987711)\nEvaluating, please wait\nzero pred_aspect_logits...\nzero pred_aspect_logits...\nzero pred_aspect_logits...\nzero pred_aspect_logits...\nzero pred_aspect_logits...\nzero pred_aspect_logits...\nzero pred_aspect_logits...\nzero pred_aspect_logits...\nzero pred_aspect_logits...\nzero pred_aspect_logits...\nzero pred_aspect_logits...\n  0%|                                                   | 0/490 [00:00<?, ?it/s]Ready to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 58.87it/s]\naspect precision: 0.8092485549132948\taspect recall: 0.24911032\taspect f1: 0.38095238\nopinion precision: 0.6673387096774194\topinion recall: 0.24943482\topinion f1: 0.36313769\nAPCE precision: 0.8870967741935484\tAPCE recall: 0.04888889\tAPCE f1: 0.09267060\npair precision: 0.3835616438356164\tpair recall: 0.02036364\tpair f1: 0.03867403\ntriple precision: 0.3835616438356164\ttriple recall: 0.02036364\ttriple f1: 0.03867403\nEvaluating complete\nEpoch:1\n100%|█████████████████████████████████████████| 571/571 [13:32<00:00,  1.42s/it]\n('Loss:', 80043.1697883606)\n('KL_Loss:', 3065.6378130746098)\nEvaluating, please wait\nzero pred_aspect_logits...\nzero pred_aspect_logits...\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 58.03it/s]\naspect precision: 0.7834710743801653\taspect recall: 0.42170819\taspect f1: 0.54829381\nopinion precision: 0.7576301615798923\topinion recall: 0.31801055\topinion f1: 0.44798301\nAPCE precision: 0.9761904761904762\tAPCE recall: 0.03644444\tAPCE f1: 0.07026564\npair precision: 0.6792452830188679\tpair recall: 0.02618182\tpair f1: 0.05042017\ntriple precision: 0.660377358490566\ttriple recall: 0.02545455\ttriple f1: 0.04901961\nEvaluating complete\nEpoch:2\n100%|█████████████████████████████████████████| 571/571 [13:31<00:00,  1.42s/it]\n('Loss:', 65491.810581207275)\n('KL_Loss:', 3032.512026593147)\nEvaluating, please wait\nzero pred_aspect_logits...\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 57.91it/s]\naspect precision: 0.7274774774774775\taspect recall: 0.57473310\taspect f1: 0.64214712\nopinion precision: 0.7535014005602241\topinion recall: 0.40542577\topinion f1: 0.52719255\nAPCE precision: 0.8090737240075614\tAPCE recall: 0.38044444\tAPCE f1: 0.51753325\npair precision: 0.562254259501966\tpair recall: 0.31200000\tpair f1: 0.40130964\ntriple precision: 0.5629139072847682\ttriple recall: 0.30909091\ttriple f1: 0.39906103\nEvaluating complete\nEpoch:3\n100%|█████████████████████████████████████████| 571/571 [13:30<00:00,  1.42s/it]\n('Loss:', 52748.891971588135)\n('KL_Loss:', 3054.248703657011)\nEvaluating, please wait\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 57.78it/s]\naspect precision: 0.7234726688102894\taspect recall: 0.60053381\taspect f1: 0.65629558\nopinion precision: 0.6991341991341992\topinion recall: 0.48681236\topinion f1: 0.57396713\nAPCE precision: 0.7560355781448539\tAPCE recall: 0.52888889\tAPCE f1: 0.62238494\npair precision: 0.5802707930367504\tpair recall: 0.43636364\tpair f1: 0.49813200\nEvaluating complete\nTest results...\ntriple precision: 0.5830039525691699\ttriple recall: 0.42909091\ttriple f1: 0.49434437\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:07<00:00, 62.71it/s]\naspect precision: 0.0\taspect recall: 0.00000000\taspect f1: 0.00000000\nopinion precision: 0.0\topinion recall: 0.00000000\topinion f1: 0.00000000\nAPCE precision: 0.0\tAPCE recall: 0.00000000\tAPCE f1: 0.00000000\npair precision: 0.0\tpair recall: 0.00000000\tpair f1: 0.00000000\ntriple precision: 0.0\ttriple recall: 0.00000000\ttriple f1: 0.00000000\nEpoch:4\n100%|█████████████████████████████████████████| 571/571 [13:29<00:00,  1.42s/it]\n('Loss:', 44223.39514350891)\n('KL_Loss:', 3108.2179478464304)\nEvaluating, please wait\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 57.89it/s]\naspect precision: 0.717948717948718\taspect recall: 0.62277580\taspect f1: 0.66698428\nopinion precision: 0.6895522388059702\topinion recall: 0.52223060\topinion f1: 0.59433962\nAPCE precision: 0.7293577981651376\tAPCE recall: 0.56533333\tAPCE f1: 0.63695543\npair precision: 0.5994475138121547\tpair recall: 0.47345455\tpair f1: 0.52905323\ntriple precision: 0.5984990619136961\ttriple recall: 0.46400000\ttriple f1: 0.52273658\nEvaluating complete\nTest results...\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 60.87it/s]\naspect precision: 0.0\taspect recall: 0.00000000\taspect f1: 0.00000000\nopinion precision: 0.0\topinion recall: 0.00000000\topinion f1: 0.00000000\nAPCE precision: 0.0\tAPCE recall: 0.00000000\tAPCE f1: 0.00000000\npair precision: 0.0\tpair recall: 0.00000000\tpair f1: 0.00000000\ntriple precision: 0.0\ttriple recall: 0.00000000\ttriple f1: 0.00000000\nEpoch:5\n100%|█████████████████████████████████████████| 571/571 [13:31<00:00,  1.42s/it]\n('Loss:', 38156.97741508484)\n('KL_Loss:', 3095.83864441299)\nEvaluating, please wait\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 57.18it/s]\naspect precision: 0.7088974854932302\taspect recall: 0.65213523\taspect f1: 0.67933272\nopinion precision: 0.6591500433651344\topinion recall: 0.57272042\topinion f1: 0.61290323\nAPCE precision: 0.7112970711297071\tAPCE recall: 0.60444444\tAPCE f1: 0.65353196\npair precision: 0.5983122362869199\tpair recall: 0.51563636\tpair f1: 0.55390625\ntriple precision: 0.5998263888888888\ttriple recall: 0.50254545\ttriple f1: 0.54689355\nEvaluating complete\nTest results...\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:07<00:00, 62.55it/s]\naspect precision: 0.0\taspect recall: 0.00000000\taspect f1: 0.00000000\nopinion precision: 0.0\topinion recall: 0.00000000\topinion f1: 0.00000000\nAPCE precision: 0.0\tAPCE recall: 0.00000000\tAPCE f1: 0.00000000\npair precision: 0.0\tpair recall: 0.00000000\tpair f1: 0.00000000\ntriple precision: 0.0\ttriple recall: 0.00000000\ttriple f1: 0.00000000\nEpoch:6\n100%|█████████████████████████████████████████| 571/571 [13:29<00:00,  1.42s/it]\n('Loss:', 34064.09617614746)\n('KL_Loss:', 3085.3286996813404)\nEvaluating, please wait\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 57.50it/s]\naspect precision: 0.745575221238938\taspect recall: 0.59964413\taspect f1: 0.66469428\nopinion precision: 0.6877394636015326\topinion recall: 0.54107008\topinion f1: 0.60565162\nAPCE precision: 0.7435294117647059\tAPCE recall: 0.56177778\tAPCE f1: 0.64000000\npair precision: 0.6263008514664143\tpair recall: 0.48145455\tpair f1: 0.54440789\ntriple precision: 0.632295719844358\ttriple recall: 0.47272727\ttriple f1: 0.54099043\nEvaluating complete\nEpoch:7\n100%|█████████████████████████████████████████| 571/571 [13:32<00:00,  1.42s/it]\n('Loss:', 30436.28248977661)\n('KL_Loss:', 3158.484812237658)\nEvaluating, please wait\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 58.09it/s]\naspect precision: 0.7584269662921348\taspect recall: 0.60053381\taspect f1: 0.67030785\nopinion precision: 0.6983805668016194\topinion recall: 0.51996986\topinion f1: 0.59611231\nAPCE precision: 0.7530266343825666\tAPCE recall: 0.55288889\tAPCE f1: 0.63762173\npair precision: 0.6522613065326633\tpair recall: 0.47200000\tpair f1: 0.54767932\ntriple precision: 0.649390243902439\ttriple recall: 0.46472727\ttriple f1: 0.54175498\nEpoch:8\nEvaluating complete\n100%|█████████████████████████████████████████| 571/571 [13:31<00:00,  1.42s/it]\n('Loss:', 27572.295385360718)\n('KL_Loss:', 3193.8784795416236)\nEvaluating, please wait\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 57.08it/s]\naspect precision: 0.7462845010615711\taspect recall: 0.62544484\taspect f1: 0.68054211\nopinion precision: 0.7053388090349076\topinion recall: 0.51770912\topinion f1: 0.59713168\nAPCE precision: 0.7376654632972323\tAPCE recall: 0.54488889\tAPCE f1: 0.62678937\npair precision: 0.659877800407332\tpair recall: 0.47127273\tpair f1: 0.54985151\ntriple precision: 0.6556701030927835\ttriple recall: 0.46254545\ttriple f1: 0.54243070\nEvaluating complete\nEpoch:9\n100%|█████████████████████████████████████████| 571/571 [13:29<00:00,  1.42s/it]\n('Loss:', 24936.729488372803)\n('KL_Loss:', 3179.3903758601573)\nEvaluating, please wait\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 56.88it/s]\naspect precision: 0.7158308751229105\taspect recall: 0.64768683\taspect f1: 0.68005605\nopinion precision: 0.6606334841628959\topinion recall: 0.55011304\topinion f1: 0.60032895\nAPCE precision: 0.7089305402425579\tAPCE recall: 0.57155556\tAPCE f1: 0.63287402\npair precision: 0.6325966850828729\tpair recall: 0.49963636\tpair f1: 0.55830963\ntriple precision: 0.6298031865042174\ttriple recall: 0.48872727\ttriple f1: 0.55036855\nEvaluating complete\nTest results...\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:07<00:00, 62.38it/s]\naspect precision: 0.0\taspect recall: 0.00000000\taspect f1: 0.00000000\nopinion precision: 0.0\topinion recall: 0.00000000\topinion f1: 0.00000000\nAPCE precision: 0.0\tAPCE recall: 0.00000000\tAPCE f1: 0.00000000\npair precision: 0.0\tpair recall: 0.00000000\tpair f1: 0.00000000\ntriple precision: 0.0\ttriple recall: 0.00000000\ttriple f1: 0.00000000\nEpoch:10\n100%|█████████████████████████████████████████| 571/571 [13:32<00:00,  1.42s/it]\n('Loss:', 22802.988432884216)\n('KL_Loss:', 3219.876821682385)\nEvaluating, please wait\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:09<00:00, 53.78it/s]\naspect precision: 0.7326633165829146\taspect recall: 0.64857651\taspect f1: 0.68806041\nopinion precision: 0.6834123222748815\topinion recall: 0.54333082\topinion f1: 0.60537364\nAPCE precision: 0.7304250559284117\tAPCE recall: 0.58044444\tAPCE f1: 0.64685488\npair precision: 0.639963167587477\tpair recall: 0.50545455\tpair f1: 0.56481105\ntriple precision: 0.6378174976481655\ttriple recall: 0.49309091\ttriple f1: 0.55619360\nEvaluating complete\nTest results...\n  0%|                                                   | 0/490 [00:00<?, ?it/s]Ready to for\n100%|█████████████████████████████████████████| 490/490 [00:07<00:00, 62.59it/s]\naspect precision: 0.0\taspect recall: 0.00000000\taspect f1: 0.00000000\nopinion precision: 0.0\topinion recall: 0.00000000\topinion f1: 0.00000000\nAPCE precision: 0.0\tAPCE recall: 0.00000000\tAPCE f1: 0.00000000\npair precision: 0.0\tpair recall: 0.00000000\tpair f1: 0.00000000\ntriple precision: 0.0\ttriple recall: 0.00000000\ttriple f1: 0.00000000\nEpoch:11\n100%|█████████████████████████████████████████| 571/571 [13:30<00:00,  1.42s/it]\n('Loss:', 20493.81089782715)\n('KL_Loss:', 3183.69800453164)\nEvaluating, please wait\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 56.71it/s]\naspect precision: 0.7350157728706624\taspect recall: 0.62188612\taspect f1: 0.67373494\nopinion precision: 0.7229050279329609\topinion recall: 0.48756594\topinion f1: 0.58235824\nAPCE precision: 0.7541191381495564\tAPCE recall: 0.52888889\tAPCE f1: 0.62173459\npair precision: 0.6800433839479393\tpair recall: 0.45600000\tpair f1: 0.54592947\ntriple precision: 0.6865342163355408\ttriple recall: 0.45236364\ttriple f1: 0.54537484\nEvaluating complete\nEpoch:12\n100%|█████████████████████████████████████████| 571/571 [13:28<00:00,  1.42s/it]\n('Loss:', 18801.351729393005)\n('KL_Loss:', 3270.175709890235)\nEvaluating, please wait\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 57.47it/s]\naspect precision: 0.7697674418604651\taspect recall: 0.58896797\taspect f1: 0.66733871\nopinion precision: 0.7240977881257276\topinion recall: 0.46872645\topinion f1: 0.56907594\nAPCE precision: 0.7526315789473684\tAPCE recall: 0.50844444\tAPCE f1: 0.60689655\npair precision: 0.6790123456790124\tpair recall: 0.44000000\tpair f1: 0.53398058\ntriple precision: 0.6842105263157895\ttriple recall: 0.43490909\ttriple f1: 0.53179191\nEpoch:13\nEvaluating complete\n100%|█████████████████████████████████████████| 571/571 [13:29<00:00,  1.42s/it]\n('Loss:', 16924.16512775421)\n('KL_Loss:', 3233.960464109602)\nEvaluating, please wait\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 57.52it/s]\naspect precision: 0.7111770524233432\taspect recall: 0.63967972\taspect f1: 0.67353630\nopinion precision: 0.7142857142857143\topinion recall: 0.49736247\topinion f1: 0.58640604\nAPCE precision: 0.7336561743341404\tAPCE recall: 0.53866667\tAPCE f1: 0.62121989\npair precision: 0.674074074074074\tpair recall: 0.46327273\tpair f1: 0.54913793\ntriple precision: 0.6720085470085471\ttriple recall: 0.45745455\ttriple f1: 0.54435309\nEvaluating complete\nEpoch:14\n100%|█████████████████████████████████████████| 571/571 [13:29<00:00,  1.42s/it]\n('Loss:', 15833.372924804688)\n('KL_Loss:', 3235.8994303604927)\nEvaluating, please wait\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 56.77it/s]\naspect precision: 0.7256281407035176\taspect recall: 0.64234875\taspect f1: 0.68145352\nopinion precision: 0.6850094876660342\topinion recall: 0.54408440\topinion f1: 0.60646787\nAPCE precision: 0.7242888402625821\tAPCE recall: 0.58844444\tAPCE f1: 0.64933791\npair precision: 0.6352833638025595\tpair recall: 0.50545455\tpair f1: 0.56298096\ntriple precision: 0.6354264292408622\ttriple recall: 0.49309091\ttriple f1: 0.55528256\nEvaluating complete\nTest results...\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:07<00:00, 62.48it/s]\naspect precision: 0.0\taspect recall: 0.00000000\taspect f1: 0.00000000\nopinion precision: 0.0\topinion recall: 0.00000000\topinion f1: 0.00000000\nAPCE precision: 0.0\tAPCE recall: 0.00000000\tAPCE f1: 0.00000000\npair precision: 0.0\tpair recall: 0.00000000\tpair f1: 0.00000000\ntriple precision: 0.0\ttriple recall: 0.00000000\ttriple f1: 0.00000000\nEpoch:15\n100%|█████████████████████████████████████████| 571/571 [13:29<00:00,  1.42s/it]\n('Loss:', 14411.241955280304)\n('KL_Loss:', 3302.97984350097)\nEvaluating, please wait\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 58.15it/s]\naspect precision: 0.7548460661345496\taspect recall: 0.58896797\taspect f1: 0.66166917\nopinion precision: 0.7105549510337323\topinion recall: 0.49208742\topinion f1: 0.58147818\nAPCE precision: 0.7652173913043478\tAPCE recall: 0.54755556\tAPCE f1: 0.63834197\npair precision: 0.6656184486373166\tpair recall: 0.46181818\tpair f1: 0.54529841\ntriple precision: 0.6663124335812965\ttriple recall: 0.45600000\ttriple f1: 0.54145078\nEvaluating complete\nEpoch:16\n100%|█████████████████████████████████████████| 571/571 [13:31<00:00,  1.42s/it]\n('Loss:', 13176.435729980469)\n('KL_Loss:', 3252.754561605191)\nEvaluating, please wait\n  0%|                                                   | 0/490 [00:00<?, ?it/s]Ready to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 57.90it/s]\naspect precision: 0.7205284552845529\taspect recall: 0.63078292\taspect f1: 0.67267552\nopinion precision: 0.6803519061583577\topinion recall: 0.52449133\topinion f1: 0.59234043\nAPCE precision: 0.7212931995540691\tAPCE recall: 0.57511111\tAPCE f1: 0.63996044\npair precision: 0.6299065420560748\tpair recall: 0.49018182\tpair f1: 0.55132924\ntriple precision: 0.6312796208530805\ttriple recall: 0.48436364\ttriple f1: 0.54814815\nEvaluating complete\nEpoch:17\n100%|█████████████████████████████████████████| 571/571 [13:34<00:00,  1.43s/it]\n('Loss:', 12551.966511249542)\n('KL_Loss:', 3252.8706731844322)\nEvaluating, please wait\n  0%|                                                   | 0/490 [00:00<?, ?it/s]Ready to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 57.13it/s]\naspect precision: 0.673972602739726\taspect recall: 0.65658363\taspect f1: 0.66516449\nopinion precision: 0.6064414768263944\topinion recall: 0.58176338\topinion f1: 0.59384615\nAPCE precision: 0.6666666666666666\tAPCE recall: 0.62755556\tAPCE f1: 0.64652015\npair precision: 0.5585106382978723\tpair recall: 0.53454545\tpair f1: 0.54626533\ntriple precision: 0.5624024960998439\ttriple recall: 0.52436364\ttriple f1: 0.54271735\nEvaluating complete\nEpoch:18\n100%|█████████████████████████████████████████| 571/571 [13:31<00:00,  1.42s/it]\n('Loss:', 11360.143110752106)\n('KL_Loss:', 3320.7462620056394)\nEvaluating, please wait\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 54.62it/s]\naspect precision: 0.6180290297937356\taspect recall: 0.71975089\taspect f1: 0.66502261\nopinion precision: 0.536144578313253\topinion recall: 0.67068576\topinion f1: 0.59591563\nAPCE precision: 0.6144674085850557\tAPCE recall: 0.68711111\tAPCE f1: 0.64876206\npair precision: 0.4919402985074627\tpair recall: 0.59927273\tpair f1: 0.54032787\ntriple precision: 0.5038119440914867\ttriple recall: 0.57672727\ttriple f1: 0.53780943\nEvaluating complete\nEpoch:19\n100%|█████████████████████████████████████████| 571/571 [13:29<00:00,  1.42s/it]\n('Loss:', 10605.392250537872)\n('KL_Loss:', 3295.576160247504)\nEvaluating, please wait\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:08<00:00, 55.97it/s]\naspect precision: 0.6336386344712739\taspect recall: 0.67704626\taspect f1: 0.65462366\nopinion precision: 0.6144101346001584\topinion recall: 0.58477769\topinion f1: 0.59922780\nAPCE precision: 0.6412556053811659\tAPCE recall: 0.63555556\tAPCE f1: 0.63839286\npair precision: 0.5413371675053918\tpair recall: 0.54763636\tpair f1: 0.54446855\ntriple precision: 0.5496240601503759\ttriple recall: 0.53163636\ttriple f1: 0.54048059\nEvaluating complete\nTest results...\n  0%|                                                   | 0/490 [00:00<?, ?it/s]Ready to for\n100%|█████████████████████████████████████████| 490/490 [00:07<00:00, 62.01it/s]\naspect precision: 0.0\taspect recall: 0.00000000\taspect f1: 0.00000000\nopinion precision: 0.0\topinion recall: 0.00000000\topinion f1: 0.00000000\nAPCE precision: 0.0\tAPCE recall: 0.00000000\tAPCE f1: 0.00000000\npair precision: 0.0\tpair recall: 0.00000000\tpair f1: 0.00000000\ntriple precision: 0.0\ttriple recall: 0.00000000\ttriple f1: 0.00000000\nFeatures build completed\nEvaluation on testset:\nReady to for\n100%|█████████████████████████████████████████| 490/490 [00:07<00:00, 61.62it/s]\naspect precision: 0.0\taspect recall: 0.00000000\taspect f1: 0.00000000\nopinion precision: 0.0\topinion recall: 0.00000000\topinion f1: 0.00000000\nAPCE precision: 0.0\tAPCE recall: 0.00000000\tAPCE f1: 0.00000000\npair precision: 0.0\tpair recall: 0.00000000\tpair f1: 0.00000000\ntriple precision: 0.0\ttriple recall: 0.00000000\ttriple f1: 0.00000000\n\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:          KL_Loss █▂▁▁▃▂▂▄▄▄▅▄▆▅▅▆▅▅▇▆\n\u001b[34m\u001b[1mwandb\u001b[0m:             Loss █▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n\u001b[34m\u001b[1mwandb\u001b[0m:         Val_Loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\u001b[34m\u001b[1mwandb\u001b[0m:        triple f1 ▁▁▆▇████████████████\n\u001b[34m\u001b[1mwandb\u001b[0m: triple precision ▁▇▅▆▆▆▇▇▇▇▇███▇█▇▅▄▅\n\u001b[34m\u001b[1mwandb\u001b[0m:    triple recall ▁▁▅▆▇▇▇▇▇▇▇▆▆▆▇▆▇▇█▇\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:          KL_Loss 3295.57616\n\u001b[34m\u001b[1mwandb\u001b[0m:             Loss 10605.39225\n\u001b[34m\u001b[1mwandb\u001b[0m:         Val_Loss 0\n\u001b[34m\u001b[1mwandb\u001b[0m:        triple f1 0.54048\n\u001b[34m\u001b[1mwandb\u001b[0m: triple precision 0.54962\n\u001b[34m\u001b[1mwandb\u001b[0m:    triple recall 0.53164\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mgolden-frost-125\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/lmartinson/aste-SBN/runs/u98be4tg\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240429_125936-u98be4tg/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # %cd /kaggle/working\n# from IPython.display import FileLink\n# FileLink('SBN_models/fake_start_base_full_model_4_0.6850574712643679.pt')","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:00:12.737735Z","iopub.execute_input":"2024-03-12T10:00:12.738474Z","iopub.status.idle":"2024-03-12T10:00:12.744784Z","shell.execute_reply.started":"2024-03-12T10:00:12.738442Z","shell.execute_reply":"2024-03-12T10:00:12.743803Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/SBN_models/fake_start_base_full_model_4_0.6850574712643679.pt","text/html":"<a href='SBN_models/fake_start_base_full_model_4_0.6850574712643679.pt' target='_blank'>SBN_models/fake_start_base_full_model_4_0.6850574712643679.pt</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"# sent\n# 116 - gcn (correct version) ~ 53\n# 117 - gcn - no spans started with punk - random_shuffle ==> ~0.5\n\n# full\n# 118 - gcn - no spans started with punk - random_shuffle - max_span_length=3 ==>\n# 125 - fixed gcn - no spans started with punk - zeros adj matrix --0.57 at 10","metadata":{},"execution_count":null,"outputs":[]}]}